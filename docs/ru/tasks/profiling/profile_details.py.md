
**Версия**: 1.0  
**Дата**: 10 марта 2025  
**Автор**: V.Khvatov

## 1. Описание и назначение

Скрипт `profile_details.py` выполняет комплексный профайлинг таблицы RESUME_DETAILS (DETAILS.csv) в рамках проекта анонимизации резюме PAMOLA.CORE (Privacy-Preserving AI Data Processors). Данный скрипт является логическим продолжением профайлинга таблицы IDENTIFICATION и использует ту же методологию, но расширяет её для анализа специфических полей, характерных для профессиональных данных резюме, включая многозначные поля (MVF - Multi-Value Fields).

Профайлинг таблицы RESUME_DETAILS необходим для:

1. Понимания структуры и качества данных профессиональной информации
2. Выявления закономерностей и аномалий в данных резюме
3. Оценки вариабельности и взаимосвязей между различными полями
4. Подготовки к последующей анонимизации данных согласно выявленным особенностям

## 2. Особенности исходных данных

### 2.1 Структура таблицы RESUME_DETAILS

Таблица RESUME_DETAILS содержит профессиональную информацию из резюме:

|Поле|Тип данных|Описание|Примеры значений|
|---|---|---|---|
|resume_id|long|Идентификатор резюме|242472615, 242472623|
|post|text|Должность|Руководитель направления по разработке, Project Manager|
|education_level|text|Уровень образования|Высшее образование, Бакалавр, Магистр|
|salary|double|Желаемая зарплата|0, 230000, 150000|
|salary_currency|text|Валюта зарплаты|RUR, USD, пустые значения|
|area_name|text|Регион/город|Москва, Магнитогорск, Санкт-Петербург|
|relocation|text|Готовность к переезду|Возможен, Невозможен, Желателен|
|metro_station_name|text|Станция метро|Сокольники, пустые значения|
|road_time_type|text|Время в пути|Не имеет значения, Не более часа|
|business_trip_readiness|text|Готовность к командировкам|Иногда, Готов, Никогда|
|work_schedules|text (MVF)|График работы|['Полный день'], ['Гибкий график', 'Удаленная работа']|
|employments|text (MVF)|Тип занятости|['Полная занятость'], ['Проектная работа', 'Частичная занятость']|
|driver_license_types|text (MVF)|Типы водительских прав|['B', 'C'], [] (пустой массив)|
|has_vehicle|boolean|Наличие личного транспорта|True, False, пустые значения|

### 2.2 Многозначные поля (MVF)

Особенностью таблицы являются многозначные поля, представленные как строковые представления массивов:

- `work_schedules`: содержит список возможных графиков работы
- `employments`: содержит список желаемых типов занятости
- `driver_license_types`: содержит список категорий водительских прав

Пример такого поля: `"['Удаленная работа', 'Полный день']"`

### 2.3 Проблема групповых идентификаторов

Записи с одинаковым `resume_id` образуют группу, относящуюся к одному резюме. Эти группы могут содержать:

- Дубликаты для сохранения связности с другими таблицами
- Различные версии резюме после редактирования
- Записи, имеющие разное значение полей при сохранении целостности данных

## 3. Архитектура и организация кода

### 3.1 Структура скрипта

Скрипт `profile_details.py` следует модульной архитектуре, разделяя логику на функциональные блоки:

1. **Импорты и настройка**: подключение необходимых модулей и логгера
2. **Специализированные функции анализа**: функции для анализа конкретных типов полей
    - `analyze_post_field`: анализ поля должности
    - `analyze_salary_field`: анализ поля зарплаты
    - `analyze_metro_station_field`: анализ поля станции метро
    - `analyze_currency_field`: анализ поля валюты
3. **Вспомогательные функции**: функции для анализа определенных групп полей
    - `analyze_mvf_fields`: анализ многозначных полей
    - `analyze_categorical_fields`: анализ категориальных полей
    - `analyze_correlation_pairs`: анализ корреляций между парами полей
4. **Основная функция**: `main()`, которая оркестрирует все аналитические операции
5. **Точка входа**: парсинг аргументов командной строки и запуск основной функции

### 3.2 Зависимости и интеграция с другими модулями

Скрипт использует функциональность из следующих модулей проекта PAMOLA.CORE:

- **Аналитические модули**:
    
    - `basic.py`: базовый анализ, включая полноту, уникальность и дубликаты
    - `categorical.py`: анализ категориальных переменных
    - `correlation_analysis.py`: корреляционный анализ полей
    - `group_variation.py`: анализ вариабельности внутри групп
    - `mvf_analysis.py`: специализированный анализ многозначных полей
    - `text_analysis.py`: анализ текстовых полей
- **Утилиты**:
    
    - `io.py`: функции ввода-вывода, включая чтение/запись файлов
    - `logging.py`: настройка логирования
    - `progress.py`: отслеживание прогресса выполнения
    - `task_reporting.py`: формирование отчетов о задачах

## 4. Функциональные возможности и методы анализа

### 4.1 Базовые метрики

#### 4.1.1 Анализ полноты данных

- Расчет процента заполненных значений для каждого поля
- Выявление полей с наибольшим количеством пропусков
- Визуализация полноты через `plot_completeness`
- Результат: `completeness.json` и `completeness.png`

#### 4.1.2 Анализ уникальности значений

- Расчет процента уникальных значений для каждого поля
- Определение кардинальности полей
- Оценка потенциального риска приватности
- Результат: `uniqueness.json`

### 4.2 Специализированный анализ полей

#### 4.2.1 Анализ поля post (Должность)

- Создание словаря уникальных значений с частотностью
- Анализ распределения топ-15 значений
- Текстовый анализ - выявление ключевых слов и их частотности
- Анализ средней длины значений
- Визуализация через облако слов
- Результаты: `post_stats.json`, `post_dictionary.csv`, `post_distribution.png`, `post_wordcloud.png`

#### 4.2.2 Анализ поля education_level (Уровень образования)

- Создание справочника значений с частотностью
- Анализ распределения значений
- Визуализация распределения
- Результаты: `education_level_stats.json`, `education_level_dictionary.csv`, `education_level_distribution.png`

#### 4.2.3 Анализ поля salary (Зарплата)

- Статистический анализ (минимум, максимум, среднее, медиана)
- Определение выбросов методом IQR
- Анализ распределения по размерным группам
- Логарифмический график распределения
- Результаты: `salary_stats.json`, `salary_groups_distribution.png`, `salary_log_distribution.png`

#### 4.2.4 Анализ поля salary_currency (Валюта зарплаты)

- Анализ пропусков и заполненности
- Распределение по типам валют
- Исследование взаимосвязи с полем salary
- Результаты: `salary_currency_stats.json`, `salary_currency_dictionary.csv`, `salary_currency_distribution.png`

#### 4.2.5 Анализ многозначных полей (MVF)

Для полей `work_schedules`, `employments`, `driver_license_types`:

- Извлечение значений из строкового представления массива
- Анализ отдельных значений и их комбинаций
- Статистика по количеству значений на запись
- Создание специализированных справочников
- Визуализация распределений значений, комбинаций и количества
- Результаты: отдельные JSON-файлы, CSV-справочники и PNG-визуализации для каждого поля

#### 4.2.6 Анализ поля has_vehicle (Наличие транспорта)

- Анализ частотности значений (True, False, NULL)
- Визуализация распределения
- Результаты: `has_vehicle_stats.json`, `has_vehicle_distribution.png`

### 4.3 Анализ вариабельности внутри групп

Для групп записей с одинаковым `resume_id` вычисляются показатели вариабельности:

#### 4.3.1 Анализ по первому набору полей

- education_level (вес 0.2)
- area_name (вес 0.6)
- employments (вес 0.2)
- Результаты: `group_variation_set1.json`, `group_variation_details_set1.csv`, `group_variation_distribution_set1.png`

#### 4.3.2 Анализ по второму набору полей

- area_name (вес 0.3)
- work_schedules (вес 0.3)
- salary (вес 0.4)
- Результаты: `group_variation_set2.json`, `group_variation_details_set2.csv`, `group_variation_distribution_set2.png`

### 4.4 Корреляционный анализ

Проведение корреляционного анализа между парами полей:

- salary и area_name
- salary и education_level
- relocation и area_name
- business_trip_readiness и relocation
- work_schedules и employments
- driver_license_types и has_vehicle

Для категориальных полей используются специализированные меры связи:

- V Крамера для двух категориальных переменных
- Точечно-бисериальная корреляция для категориальной и числовой переменных
- Корреляционное отношение для категориальной и непрерывной переменных

Результаты:

- Отдельные JSON-файлы для каждой пары
- Корреляционная матрица `correlation_matrix.json`
- Тепловая карта `correlation_matrix_heatmap.png`

### 4.5 Анализ дубликатов и групп resume_id

- Выявление полных и частичных дубликатов
- Статистика по размерам групп с одинаковым resume_id
- Распределение количества записей на одно резюме
- Результаты: `duplicates.json`, `resume_id_groups.json`, `resume_id_group_size_distribution.png`

## 5. Обработка многозначных полей (MVF)

### 5.1 Парсинг значений

В скрипте используется функция `parse_mvf` для извлечения значений из строкового представления массива:

```python
def parse_mvf(value):
    """
    Извлекает значения из строкового представления массива.
    """
    if pd.isna(value) or value == '[]':
        return []
    
    # Проверка типа значения
    if isinstance(value, bool):
        return [str(value)]  # Преобразуем bool в строку
    
    # Если значение не является строкой, преобразуем его в строку
    if not isinstance(value, str):
        return [str(value)]

    try:
        # Пробуем использовать ast.literal_eval для безопасного парсинга
        parsed_value = ast.literal_eval(value)
        if isinstance(parsed_value, list):
            return parsed_value
        else:
            logger.warning(f"Parsed value is not a list: {value}")
            return []
    except (SyntaxError, ValueError) as e:
        # "Ручной" парсинг как запасной вариант
        try:
            cleaned = value.strip('[]')
            if not cleaned:
                return []

            items = []
            for item in cleaned.split(','):
                cleaned_item = item.strip().strip("'\"")
                if cleaned_item:
                    items.append(cleaned_item)

            return items
        except Exception as e2:
            logger.error(f"Error parsing MVF value '{value}': {e2}")
            return []
```

### 5.2 Анализ отдельных значений и комбинаций

Для многозначных полей выполняется:

1. Анализ частоты появления отдельных значений
2. Анализ частоты появления комбинаций значений
3. Анализ распределения количества значений на запись

Это позволяет понять:

- Какие значения и комбинации наиболее популярны
- Сколько значений обычно указывают пользователи
- Какие комбинации встречаются вместе (например, какие типы занятости сочетаются)

## 6. Корреляционный анализ между полями

### 6.1 Методы анализа корреляций

В зависимости от типов переменных используются разные методы:

1. **Для двух числовых переменных**:
    
    - Корреляция Пирсона (стандартная линейная корреляция)
2. **Для двух категориальных переменных**:
    
    - V Крамера (основан на статистике хи-квадрат)
3. **Для категориальной и числовой переменных**:
    
    - Точечно-бисериальная корреляция (если категория бинарная)
    - Корреляционное отношение (для многозначной категории и числовой переменной)

### 6.2 Интерпретация корреляций

Для каждого коэффициента корреляции предоставляется текстовая интерпретация:

```python
def interpret_correlation(correlation_value, method):
    # Берем абсолютное значение корреляции для интерпретации
    abs_corr = abs(correlation_value)
    
    # Различные шкалы интерпретации в зависимости от метода
    if method in ['cramers_v', 'correlation_ratio']:
        if abs_corr < 0.1:
            strength = "Negligible association"
        elif abs_corr < 0.2:
            strength = "Weak association"
        # И так далее...
    else:  # Для Пирсона и точечно-бисериальной корреляции
        if abs_corr < 0.1:
            strength = "Negligible correlation"
        # И так далее...
    
    # Добавляем направление для методов с отрицательными значениями
    if method in ['pearson', 'point_biserial']:
        direction = "positive" if correlation_value >= 0 else "negative"
        return f"{strength} ({direction})"
    else:
        return strength
```

## 7. Анализ вариабельности групп

### 7.1 Подход к расчету вариабельности

Для групп записей с одинаковым `resume_id` вычисляется степень их различия:

1. **Для одного поля**:
    
    ```
    field_variation = (number_of_unique_values - 1) / (number_of_records - 1)
    ```
    
2. **Взвешенная вариабельность по нескольким полям**:
    
    ```
    weighted_variation = (
        field1_variation * weight1 + 
        field2_variation * weight2 + 
        ...
    ) / total_weight
    ```
    

Значение вариабельности находится в диапазоне [0, 1]:

- 0: все значения в группе одинаковые
- 1: все значения в группе различны

### 7.2 Наборы полей для анализа вариабельности

Скрипт анализирует вариабельность по двум наборам полей:

1. **Базовые характеристики резюме**:
    
    - education_level (вес 0.2)
    - area_name (вес 0.6)
    - employments (вес 0.2)
2. **Профессиональные предпочтения**:
    
    - area_name (вес 0.3)
    - work_schedules (вес 0.3)
    - salary (вес 0.4)

Эти два набора позволяют понять, какие аспекты резюме наиболее подвержены изменениям.

## 8. Выходные данные

### 8.1 Структура выходных данных

```
data/processed/profiling/details/
├── completeness.json                      # Полнота данных
├── completeness_*.png                     # Визуализация полноты
├── uniqueness.json                        # Уникальность значений
├── post_stats.json                        # Анализ поля должности
├── post_distribution_*.png                # Визуализация распределения должностей
├── post_wordcloud_*.png                   # Облако слов для ключевых слов должностей
├── education_level_stats.json             # Анализ уровня образования
├── salary_stats.json                      # Статистика по зарплате
├── salary_groups_distribution_*.png       # Распределение зарплат по группам
├── salary_log_distribution_*.png          # Логарифмическое распределение зарплат
├── correlation_matrix.json                # Корреляционная матрица
├── correlation_matrix_heatmap_*.png       # Тепловая карта корреляций
├── group_variation_set1.json              # Анализ вариабельности по 1-му набору
├── group_variation_set2.json              # Анализ вариабельности по 2-му набору
├── dictionaries/                          # Директория со справочниками
│   ├── post_dictionary.csv                # Справочник должностей
│   ├── education_level_dictionary.csv     # Справочник уровней образования
│   ├── work_schedules_values_dictionary.csv  # Справочник значений графиков работы
│   ├── work_schedules_combinations_dictionary.csv  # Справочник комбинаций графиков
│   └── ...                                # Другие справочники
```

### 8.2 Отчет о выполнении задачи

Отчет о выполнении задачи сохраняется в `data/reports/details_report.json` и содержит:

- Метаданные о задаче (ID, описание, время выполнения)
- Список выполненных операций с их статусами
- Информацию о созданных артефактах (файлах)
- Сведения о возникших ошибках и предупреждениях
- Информацию об использованных ресурсах

## 9. Использование скрипта

### 9.1 Аргументы командной строки

|Аргумент|Тип|По умолчанию|Описание|
|---|---|---|---|
|`--input`|str|'data/raw/DETAILS.csv'|Путь к входному CSV-файлу с данными|
|`--output`|str|None|Путь к директории для выходных файлов|
|`--log`|str|'profile_details.log'|Путь к файлу логов|
|`--encoding`|str|'utf-8'|Кодировка входного файла|
|`--delimiter`|str|';'|Разделитель полей в CSV-файле|
|`--quotechar`|str|'"'|Символ кавычек в CSV-файле|
|`--escapechar`|str|None|Символ экранирования в CSV-файле|
|`--on_bad_lines`|str|'warn'|Действие при обнаружении проблемных строк|

### 9.2 Базовый запуск

```bash
python scripts/profile_details.py
```

### 9.3 Запуск с пользовательскими параметрами

```bash
python scripts/profile_details.py --input data/custom/mydata.csv --output data/custom_output --encoding utf-8 --delimiter "," --quotechar "'"
```

## 10. Технические особенности реализации

### 10.1 Обработка ошибок и логирование

Скрипт использует иерархическую систему обработки ошибок:

1. Каждая функция анализа обрабатывает свои исключения
2. Все ошибки логируются с подробным описанием
3. Информация об ошибках добавляется в отчет о задаче

Пример обработки ошибок в функции анализа:

```python
try:
    logger.info(f"Analyzing {field} field")
    reporter.add_operation(f"Analyzing {field} field")

    field_stats = analyze_categorical_field(df, field)
    field_path = save_profiling_results(field_stats, 'details', f'{field}_stats')
    reporter.add_artifact("json", field_path, f"{field} analysis")
    
    # ... дополнительный код ...
    
except Exception as e:
    logger.error(f"Error analyzing field {field}: {e}", exc_info=True)
    reporter.add_operation(f"Analyzing {field} field", status="error",
                           details={"error": str(e)})
```

### 10.2 Отслеживание прогресса и формирование отчетов

Скрипт использует контекстные менеджеры для отслеживания задач:

```python
with TaskReporter(task_id, task_description, "profiling", script_path) as reporter:
    # ... операции профайлинга ...
    
    with track_operation("Analyzing completeness and uniqueness", total=2) as tracker:
        # Анализ полноты
        completeness = analyze_completeness(df)
        completeness_path = save_profiling_results(completeness, 'details', 'completeness')
        reporter.add_artifact("json", completeness_path, "Completeness analysis")
        tracker.update(1)
        
        # ... другие операции ...
```

### 10.3 Работа с JSON-сериализацией

Для корректной сериализации в JSON используется преобразование специальных типов:

```python
def convert_numpy_types(obj):
    """Рекурсивно преобразует типы данных NumPy в стандартные типы Python."""
    if isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(v) for v in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    # ... другие конверсии ...
```

## 11. Результаты анализа

Скрипт `profile_details.py` создает серию аналитических артефактов, которые помогают понять:

1. **Структуру данных резюме**:
    
    - Полнота и уникальность полей
    - Распределение значений категориальных полей
    - Специфика многозначных полей
2. **Качество данных**:
    
    - Выявление пропусков и аномалий
    - Определение выбросов в числовых полях
    - Выявление дубликатов
3. **Взаимосвязи между полями**:
    
    - Корреляции между профессиональными характеристиками
    - Связь между предпочтениями по зарплате и регионом
    - Связь между наличием транспорта и водительскими правами
4. **Изменчивость данных**:
    
    - Какие поля наиболее подвержены изменениям
    - Какие поля обычно остаются стабильными
    - Насколько сильно варьируются записи одного резюме

Эти результаты являются основой для:

- Выбора методов анонимизации различных полей
- Определения полей, требующих специальной обработки
- Понимания взаимосвязей, которые необходимо сохранить при анонимизации

## 12. Заключение

Скрипт `profile_details.py` предоставляет комплексный инструментарий для анализа профессиональных данных резюме, учитывающий их специфику, включая многозначные поля и групповую структуру. Модульная архитектура и интеграция с другими компонентами проекта PAMOLA.CORE обеспечивают расширяемость и поддерживаемость решения.

Основные преимущества скрипта:

1. **Специализированный анализ** для разных типов полей
2. **Работа с многозначными полями** (MVF)
3. **Анализ корреляций** между различными профессиональными характеристиками
4. **Оценка вариабельности** внутри групп с одинаковым resume_id
5. **Структурированная отчетность** о выполнении задачи профайлинга

Результаты профайлинга RESUME_DETAILS являются ключевым звеном в пайплайне анонимизации данных резюме, обеспечивая понимание специфики данных перед применением методов обезличивания.