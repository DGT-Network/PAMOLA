# PAMOLA.CORE/scripts/profile_ident.py: Профайлинг идентификационных данных

**Версия**: 1.1  
**Дата**: 10 марта 2025  
**Автор**: V.Khvatov  
**Модификации**: Добавлены метрики вариабельности групп, механизм отчетности задач

## 1. Описание и назначение

Скрипт `profile_ident.py` выполняет комплексный профайлинг таблицы IDENTIFICATION в рамках проекта PAMOLA.CORE (Privacy-Preserving AI Data Processors). Этот анализ является первым шагом в подготовке к анонимизации данных резюме, позволяя выявить закономерности, аномалии и особенности структуры данных.

Особенностью структуры данных PAMOLA.CORE (Privacy-Preserving AI Data Processors) является наличие множественных записей с одинаковым `resume_id`, представляющих собой различные версии и элементы одного резюме. Это создает дополнительные сложности при анализе и требует специфических подходов к измерению вариабельности данных внутри групп записей.

## 2. Архитектура и организация кода

### 2.1 Структура проекта
```
PAMOLA.CORE/
├── configs/             # Конфигурационные файлы
├── data/                # Репозиторий данных
│   ├── raw/             # Исходные данные
│   ├── processed/       # Обработанные данные
│   │   └── profiling/   # Результаты профайлинга
│   ├── logs/            # Файлы логов
│   └── reports/         # Отчеты о выполнении задач
├── pamola_core/                 # Основной пакет
│   ├── profiling/       # Модули профайлинга
│   │   ├── basic.py             # Базовые функции профайлинга
│   │   ├── categorical.py       # Анализ категориальных данных
│   │   ├── date_analysis.py     # Анализ дат
│   │   ├── identity_analysis.py # Анализ идентификаторов
│   │   ├── group_variation.py   # Анализ вариабельности групп
│   │   └── visualization.py     # Функции визуализации
│   ├── cleaning/        # Модули очистки данных (будущее)
│   ├── anonymization/   # Модули анонимизации (будущее)
│   └── utils/           # Утилиты
│       ├── io.py               # Функции ввода-вывода
│       ├── logging.py          # Функции логирования
│       ├── progress.py         # Отслеживание прогресса
│       └── task_reporting.py   # Формирование отчетов о задачах
└── scripts/             # Скрипты для выполнения задач
    └── profile_ident.py        # Скрипт профайлинга таблицы IDENTIFICATION
```

### 2.2 Модульная организация
Код организован в виде модульной структуры, где:
- **Скрипты** (в директории `scripts/`) определяют конкретные задачи и используют функции из базовых модулей
- **Модули профайлинга** (в `pamola_core/profiling/`) содержат функции для анализа различных аспектов данных
- **Утилиты** (в `pamola_core/utils/`) предоставляют общие функции для ввода-вывода, логирования и т.д.

Такая организация обеспечивает переиспользуемость кода и упрощает разработку новых задач профайлинга.

## 3. Входные данные

### 3.1 Аргументы командной строки

| Аргумент     | Тип | По умолчанию       | Описание                                 |
|--------------|-----|--------------------|-----------------------------------------|
| `--input`    | str | 'data/raw/IDENT.csv' | Путь к входному CSV-файлу с данными    |
| `--output`   | str | None               | Путь к директории для выходных файлов    |
| `--log`      | str | 'pamola_profiling.log' | Путь к файлу логов                      |
| `--encoding` | str | 'utf-8'           | Кодировка входного файла                 |
| `--delimiter`| str | ','                | Разделитель полей в CSV-файле            |

### 3.2 Структура входной таблицы IDENTIFICATION

Таблица IDENTIFICATION содержит следующие поля:
- `resume_id` - идентификатор резюме
- `first_name` - имя
- `last_name` - фамилия
- `middle_name` - отчество (опционально)
- `birth_day` - дата рождения
- `gender` - пол
- `UID` - уникальный идентификатор человека
- `file_as` - комбинированное поле (обычно "фамилия имя")

### 3.3 Особенности структуры данных

Каждое резюме (определяемое по `resume_id`) может иметь несколько записей в таблице, которые представляют собой:
- Различные версии резюме (внесенные изменения)
- Различные элементы опыта работы
- Различные элементы образования

Это создает специфическую структуру данных, где группы записей с одинаковым `resume_id` должны анализироваться как единое целое, с учетом вариабельности внутри группы.

## 4. Реализованные методы анализа

### 4.1 Базовые метрики

#### 4.1.1 Анализ полноты данных
- Функция `analyze_completeness` из модуля `basic.py`
- Рассчитывает процент заполненных значений для каждого поля
- Визуализация через `plot_completeness`
- Результат: `completeness.json` и `completeness.png`

#### 4.1.2 Анализ уникальности значений
- Функция `analyze_uniqueness` из модуля `basic.py`
- Рассчитывает процент уникальных значений для каждого поля
- Результат: `uniqueness.json`

### 4.2 Анализ персональных данных

#### 4.2.1 Анализ имен, фамилий и отчеств
- Функция `analyze_name_fields` из модуля `categorical.py`
- Частотное распределение имен, фамилий и отчеств
- Сохранение полных справочников с частотой встречаемости
- Визуализация через `plot_value_distribution`
- Результаты: `name_fields_analysis.json`, словари в поддиректории `dictionaries/`, визуализации топ-20 значений

#### 4.2.2 Анализ распределения полов
- Функция `analyze_frequency` из модуля `basic.py`
- Частотное распределение значений поля gender
- Визуализация через `plot_value_distribution`
- Результат: `gender_distribution.json` и `gender_distribution.png`

#### 4.2.3 Расширенный анализ дат рождения
- Функция `analyze_birth_dates` из модуля `date_analysis.py`
- Статистика валидности дат, распределение по годам и возрастным группам
- Выявление аномальных дат (слишком старые, будущие, некорректный формат)
- Проверка изменений дат в пределах одного resume_id
- Визуализация через `plot_date_distribution`
- Результат: `birth_date_stats.json`, `birth_year_distribution.png`, `birth_date_anomalies.csv`

### 4.3 Анализ идентификаторов и связей

#### 4.3.1 Анализ количества резюме на человека
- Функция `analyze_resume_counts` из модуля `identity_analysis.py`
- Функция `count_resumes_per_person` из модуля `basic.py` (альтернативный анализ)
- Статистика по количеству резюме у каждого человека
- Визуализация через `plot_resume_distribution`
- Результат: `resume_counts.json`, `name_based_resume_counts.json`, соответствующие визуализации

#### 4.3.2 Анализ UID и его консистентности
- Функции `analyze_uid_consistency` и `check_uid_generation` из модуля `identity_analysis.py`
- Проверка согласованности UID с полями, по которым он должен формироваться
- Проверка алгоритма генерации UID (на основе MD5 хеша от комбинации полей)
- Результат: `uid_consistency.json`, `uid_generation_check.json`

#### 4.3.3 Анализ поля file_as
- Функция `analyze_file_as_field` из модуля `identity_analysis.py`
- Проверка соответствия ожидаемому формату (фамилия + пробел + имя)
- Результат: `file_as_analysis.json`

#### 4.3.4 Анализ дубликатов
- Функция `analyze_duplicates` из модуля `basic.py`
- Выявление дубликатов по полям, идентифицирующим человека
- Выявление дубликатов по UID и resume_id
- Результат: `person_duplicates.json`, `uid_duplicates.json`, `resume_id_duplicates.json`

### 4.4 Анализ вариабельности групп (новое)

#### 4.4.1 Расчет вариабельности внутри групп по resume_id
- Функция `analyze_resume_group_variation` из модуля `group_variation.py`
- Рассчитывает вариабельность значений внутри групп с одинаковым resume_id
- Используются взвешенные метрики для различных полей:
  - `first_name`: вес 0.2
  - `last_name`: вес 0.4
  - `birth_day`: вес 0.4
- Визуализация через `plot_group_variation_distribution`
- Результат: `resume_group_variation.json`, детальные данные в `dictionaries/resume_group_variation_details.csv`, визуализация распределения

Пример расчета вариабельности для группы:
```python
# Для одного поля
field_variation = (number_of_unique_values - 1) / (number_of_records - 1)

# Взвешенная вариабельность по нескольким полям
weighted_variation = (
    field1_variation * weight1 + 
    field2_variation * weight2 + 
    ...
) / total_weight
```

Значение вариабельности находится в диапазоне [0, 1], где:
- 0: все значения в группе одинаковые
- 1: все значения в группе различны

## 5. Отчетность о выполнении задач (новое)

### 5.1 Механизм отчетности задач

В проект добавлен новый модуль `task_reporting.py`, который обеспечивает:
- Создание структурированных отчетов о выполнении задач
- Отслеживание метаданных (дата, время, параметры запуска)
- Регистрацию выполненных операций и их статуса
- Учет созданных артефактов (файлов JSON, CSV, PNG)
- Статистику выполнения (время, использованные ресурсы)

### 5.2 Пример использования TaskReporter

```python
# Инициализация отчета о задаче
with TaskReporter(task_id, task_description, "profiling", script_path) as reporter:
    # Добавление информации об операции
    reporter.add_operation("Operation name", details={
        "param1": value1,
        "param2": value2
    })
    
    # Регистрация созданного артефакта
    reporter.add_artifact("json", path_to_json, "Description of artifact")
    
    # Обработка ошибок
    try:
        # Выполнение операции
        ...
    except Exception as e:
        reporter.add_operation("Operation name", status="error", 
                             details={"error": str(e)})
```

### 5.3 Формат отчета о задаче

Отчеты сохраняются в директории `data/reports/` в формате JSON:

```json
{
  "task_id": "identification",
  "task_description": "Profiling of IDENTIFICATION table",
  "task_type": "profiling",
  "script_path": "...",
  "system_info": { ... },
  "start_time": "2025-03-10 17:06:55",
  "end_time": "2025-03-10 17:07:06",
  "execution_time_seconds": 11.62,
  "status": "completed",
  "operations": [
    {
      "operation": "Initialization",
      "status": "success",
      "timestamp": "2025-03-10 17:06:55",
      "details": { ... }
    },
    ... другие операции ...
  ],
  "artifacts": [
    {
      "type": "json",
      "path": "...",
      "filename": "completeness.json",
      "size_bytes": 1024,
      "timestamp": "2025-03-10 17:06:57",
      "description": "Completeness analysis results"
    },
    ... другие артефакты ...
  ],
  "errors": [],
  "warnings": [],
  "memory_usage_mb": 288.6
}
```

### 5.4 Типичные проблемы и решения

#### 5.4.1 Сериализация специальных типов данных
Проблема: При сохранении отчета в JSON могут возникать ошибки сериализации специальных типов (например, numpy.int64).

Решение: Функция `convert_numpy_types` для преобразования numpy-типов в стандартные типы Python перед сериализацией:

```python
def convert_numpy_types(obj):
    """Рекурсивно преобразует типы данных NumPy в стандартные типы Python."""
    import numpy as np

    if isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(v) for v in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return convert_numpy_types(obj.tolist())
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif obj is None or isinstance(obj, (bool, int, float, str)):
        return obj
    else:
        return str(obj)
```

## 6. Выходные данные и артефакты

### 6.1 Структура выходных данных

```
data/processed/profiling/identification/
├── completeness.json                      # Полнота данных
├── completeness_20250310_170657.png      # Визуализация полноты
├── uniqueness.json                        # Уникальность значений
├── name_fields_analysis.json              # Анализ имен и фамилий
├── first_name_distribution_*.png          # Визуализация распределения имен
├── last_name_distribution_*.png           # Визуализация распределения фамилий
├── middle_name_distribution_*.png         # Визуализация распределения отчеств
├── gender_distribution.json               # Распределение полов
├── gender_distribution_*.png              # Визуализация распределения полов
├── birth_date_stats.json                  # Статистика дат рождения
├── birth_year_distribution_*.png          # Визуализация распределения по годам
├── birth_date_anomalies_*.csv             # Список аномалий в датах
├── resume_counts.json                     # Количество резюме на человека
├── resume_count_distribution_*.png        # Визуализация распределения резюме
├── name_based_resume_counts.json          # Альтернативный анализ количества резюме
├── name_based_resume_distribution_*.png   # Визуализация альтернативного анализа
├── uid_consistency.json                   # Анализ консистентности UID
├── uid_generation_check.json              # Проверка генерации UID
├── file_as_analysis.json                  # Анализ поля file_as
├── person_duplicates.json                 # Анализ дубликатов по персональным данным
├── uid_duplicates.json                    # Анализ дубликатов по UID
├── resume_id_duplicates.json              # Анализ дубликатов по resume_id
├── resume_group_variation.json            # Анализ вариабельности групп (новое)
├── resume_group_variation_distribution_*.png  # Визуализация распределения вариабельности
└── dictionaries/                          # Директория со справочниками
    ├── first_name_dictionary.csv          # Справочник имен с частотностью
    ├── last_name_dictionary.csv           # Справочник фамилий с частотностью
    ├── middle_name_dictionary.csv         # Справочник отчеств с частотностью
    └── resume_group_variation_details.csv # Детальные данные по вариабельности (новое)
```

### 6.2 Формат отчетов о задачах

```
data/reports/
└── identification_report.json             # Отчет о выполнении задачи профайлинга
```

## 7. Как использовать скрипт

### 7.1 Базовый запуск

```bash
python scripts/profile_ident.py
```

### 7.2 С указанием параметров

```bash
python scripts/profile_ident.py --input data/raw/IDENT.csv --output data/custom_output --encoding utf-8 --delimiter ";"
```

### 7.3 Требования

Перед запуском необходимо убедиться, что:
1. Установлены все необходимые зависимости (`pandas`, `matplotlib`, `numpy`, `tqdm`)
2. Создана структура директорий проекта
3. Входной файл доступен по указанному пути

## 8. Адаптация для других таблиц

При адаптации скрипта для профайлинга других таблиц рекомендуется:

1. **Создать новый скрипт** в директории `scripts/`, например, `profile_details.py`
2. **Скопировать базовую структуру** из `profile_ident.py`, сохраняя общую логику и структуру
3. **Адаптировать операции анализа** под специфические поля таблицы
4. **Использовать task_id** соответствующий названию таблицы (например, "details")
5. **Учитывать специфические типы данных** в новой таблице (например, длинные текстовые поля)

Пример адаптации:

```python
# Инициализация отчета о задаче
with TaskReporter("details", "Profiling of RESUME_DETAILS table", "profiling", script_path) as reporter:
    # Общие операции (чтение данных и т.д.)
    ...
    
    # Специфические операции для таблицы DETAILS
    try:
        logger.info("Analyzing salary distribution...")
        salary_stats = analyze_numeric_stats(df, 'salary')
        salary_path = save_profiling_results(salary_stats, 'details', 'salary_stats')
        reporter.add_artifact("json", salary_path, "Salary statistics")
        ...
    except Exception as e:
        logger.error(f"Error in salary analysis: {e}")
        reporter.add_operation("Salary analysis", status="error", details={"error": str(e)})
```

## 9. Технические решения и особенности реализации

### 9.1 Работа с группами записей

Для анализа групп записей с одинаковым `resume_id` используется подход группировки с помощью `pandas.groupby()`:

```python
# Группировка по resume_id
groups = df.groupby('resume_id')

# Обработка каждой группы
for group_id, group_df in groups:
    # Анализ вариабельности группы
    variation = calculate_weighted_variation(group_df, fields_weights)
    ...
```

### 9.2 Обработка отсутствующих значений

При расчете вариабельности и других метрик отсутствующие значения (`None`, `NaN`) обрабатываются как отдельные значения:

```python
# В функции calculate_field_variation
values = group[field].fillna('__NA__')
```

### 9.3 Параллельная обработка

Текущая версия не использует параллельную обработку, но для больших наборов данных можно рассмотреть:
- Использование `multiprocessing` для параллельной обработки групп
- Применение обработки данных чанками через `read_csv_in_chunks` из модуля `io.py`

### 9.4 Работа с JSON-сериализацией

Для корректной сериализации в JSON используется преобразование специальных типов:

```python
# Преобразуем numpy-типы в стандартные типы Python
report = convert_numpy_types(report)

# Затем сериализуем в JSON
json.dump(report, f, ensure_ascii=False, indent=2)
```

## 10. Следующие шаги и улучшения

### 10.1 Расширение функциональности профайлинга

- **Расчет метрик k-анонимности** для выявления потенциальных рисков приватности
- **Статистический анализ корреляций** между различными полями
- **Интеграция алгоритмов машинного обучения** для выявления скрытых паттернов

### 10.2 Оптимизация производительности

- **Параллельная обработка** для ускорения анализа больших таблиц
- **Оптимизация памяти** при работе с очень большими наборами данных
- **Кэширование промежуточных результатов** для ускорения повторного анализа

### 10.3 Расширение отчетности

- **Интерактивные дашборды** для визуализации результатов профайлинга
- **HTML-отчеты** с возможностью навигации и фильтрации
- **Сравнение результатов** между разными запусками профайлинга

### 10.4 Интеграция в пайплайн анонимизации

- **Использование результатов профайлинга** для автоматизации выбора методов анонимизации
- **Автоматический выбор параметров** для методов генерализации, шумоподавления и т.д.
- **Валидация результатов анонимизации** на основе критериев, выявленных в процессе профайлинга

## 11. Заключение

Скрипт `profile_ident.py` реализует комплексный подход к профайлингу идентификационных данных в контексте подготовки к анонимизации. Ключевые особенности реализации:

1. **Модульная архитектура** для обеспечения повторного использования кода
2. **Комплексный набор метрик** для различных аспектов данных
3. **Работа с группами записей** для учета специфики структуры данных PAMOLA.CORE (Privacy-Preserving AI Data Processors)
4. **Расчет вариабельности групп** для понимания изменчивости данных внутри резюме
5. **Структурированная отчетность** о выполнении задачи профайлинга

Эти решения позволяют не только выполнить детальный анализ таблицы IDENTIFICATION, но и создают фундамент для профайлинга других таблиц и дальнейшей интеграции результатов в пайплайн анонимизации данных.