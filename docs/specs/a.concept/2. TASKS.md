# Software Requirements Specification: PAMOLA.CORE Task and Operation System

## 1. Introduction

### 1.1 Purpose and Scope

This specification defines the requirements for the Task and Operation system within the PAMOLA.CORE project (referred to as {project_root}). The PAMOLA.CORE project is focused on data anonymization, primarily for resume data, with emphasis on profiling, transformation, anonymization, and attack simulation.

This SRS establishes the architecture, interfaces, and behaviors needed to create a flexible, reusable framework that separates user-facing scripts (Tasks) from pamola_core data processing units (Operations), allowing for consistent application across multiple domains while maintaining library independence.

### 1.2 Project Overview

The PAMOLA.CORE project aims to create a comprehensive framework for anonymizing sensitive data while:

- Evaluating the quality and characteristics of data through profiling
- Applying various anonymization techniques (suppression, generalization, masking, synthetic data, etc.)
- Testing anonymized data through simulated attacks
- Ensuring proper data transformation and cleaning

The system follows a structured approach of Projects, Tasks, Operations, and Functions:

- **Projects**: Data Science activities aimed at creating and verifying privacy guarantees
- **Tasks**: Sets of operations bound to data sources that analyze or transform data
- **Operations**: Units of work that process fields or datasets to produce metrics, visualizations, or transformations
- **Functions**: Individual components that contribute to operations

### 1.3 Definitions and Acronyms

- **PAMOLA.CORE**: The root project directory ({project_root})
- **Task**: User-defined script that orchestrates a sequence of operations
- **Operation**: Pamola Core processing unit that performs specific analysis or transformation
- **Pipeline**: A sequence of interconnected tasks
- **Data Repository**: Storage location for all data assets
- **MVF**: Multi-valued field
- **SRS**: Software Requirements Specification (this document)
- **MVP**: Minimum Viable Product

## 2. System Architecture

### 2.1 High-Level Architecture

```
PAMOLA.CORE/  # {project_root}
├── configs/                            # Configuration files for projects
│   └── prj_config.json                    # Main project configuration
├── pamola_core/                               # Pamola Core library package
│   ├── profiling/                         # Profiling operations
│   ├── anonymization/                     # Anonymization operations
│   ├── transformation/                    # Data transformation operations
│   ├── attacks/                           # Attack simulation operations
│   └── utils/                             # Utility functions and base classes
│       ├── io.py                             # Data input/output utilities
│       ├── logging.py                        # Logging utilities
│       ├── progress.py                       # Progress tracking
│       ├── task_reporting.py                 # Task reporting utilities
│       ├── cli.py                            # Command-line interface utilities
│       └── ops/                              # Operation utilities
│           ├── op_base.py                       # Base operation classes
│           ├── op_data_source.py                # Data source abstractions
│           ├── op_registry.py                   # Operation registry
│           └── op_result.py                     # Operation result classes
├── scripts/                            # User scripts
│   └── pipeline/                          # Task scripts
│       ├── profiling/                        # Profiling tasks
│       ├── anonymization/                    # Anonymization tasks
│       └── attacks/                          # Attack simulation tasks
└── data/                               # Data repository
    ├── raw/                               # Raw input data
    ├── processed/                         # Processed data (by task type/id)
    ├── reports/                           # Task execution reports
    ├── logs/                              # Execution logs
    └── keys/                              # Encryption keys
```

### 2.2 Component Relationships

Tasks invoke Operations, which produce Artifacts (metrics, visualizations, transformed data) that are stored in the Data Repository. Tasks also generate reports summarizing their execution.

Operations are grouped by domain (profiling, anonymization, etc.) and may be field-specific, dataset-specific, or cross-dataset operations.

## 3. Task Requirements

### 3.1 Task Definition

A Task is a user-defined script that orchestrates a sequence of Operations to achieve a specific goal, such as profiling a dataset, anonymizing data, or simulating an attack.

### 3.2 Task Structure

Each Task must:

1. Inherit from a `BaseTask` class defined in `pamola_core/utils/tasks/base_task.py`
2. Be located in `{project_root}/scripts/pipeline/{task_type}/{task_name}.py`
3. Define its `task_id`, `task_type`, and `description`
4. Specify input and output datasets
5. Define a sequence of operations to be executed

### 3.3 Task Properties

|Property|Description|Required|Default|
|---|---|---|---|
|`task_id`|Unique identifier for the task|Yes|-|
|`task_type`|Type of task (profiling, anonymization, etc.)|Yes|-|
|`description`|Description of the task's purpose|Yes|-|
|`input_datasets`|Input datasets (paths or DataFrames)|Yes|-|
|`auxiliary_datasets`|Additional datasets used by operations|No|None|
|`output_directory`|Directory for task output|No|`{data_repository}/processed/{task_type}/{task_id}`|
|`report_path`|Path for task report|No|`{data_repository}/reports/{task_type}/{task_id}.json`|
|`operations`|List of operations to execute|Yes|-|

### 3.4 Task Lifecycle

1. **Initialization**: Load configuration, validate inputs, prepare directories
2. **Execution**: Execute operations in sequence
3. **Reporting**: Generate comprehensive execution report
4. **Cleanup**: Release resources, close files

### 3.5 Task Configuration

Tasks receive configuration from:

1. Project configuration file (`{project_root}/configs/prj_config.json`)
2. Command-line arguments (parsed using `pamola_core/utils/cli.py`)
3. Default values defined in the task class

### 3.6 Task Execution Flow

```
1. Parse command-line arguments
2. Load project configuration
3. Initialize task with parameters
4. Validate input datasets and fields
5. Create output directories if needed
6. For each operation:
   a. Prepare operation parameters
   b. Execute operation
   c. Record results and artifacts
   d. Pass relevant outputs to next operation if needed
7. Generate comprehensive task report
8. Return success/failure status
```

### 3.7 Task Output Directory Structure

```
{data_repository}/processed/{task_type}/{task_id}/
├── *.json                  # Metrics and analysis results
├── *.png                   # Visualizations
├── dictionaries/           # Extracted dictionaries and lookup tables
│   └── *.csv               # Field-specific dictionaries
└── output/                 # Transformed datasets (if any)
    └── *.csv               # Output datasets
```

### 3.8 Task Reporting

Tasks must generate a comprehensive report in JSON format stored at `{data_repository}/reports/{task_type}/{task_id}.json` containing:

1. Task metadata (id, type, description, script path)
2. System information (OS, Python version, user, machine)
3. Execution timestamps (start, end, duration)
4. Execution status  (completed, not started, running, failed)
5. For each operation:
    - Operation name
    - Status (success, failed, can't start)
    - Timestamp
    - Parameters
    - Generated artifacts (metrics, diagrams, dictionaries, output data - could be skipped or encrypted)
    - Execution time
6. Overall metrics (rows processed, artifacts generated, etc.)
7. Errors and warnings

## 4. Operation Requirements

### 4.1 Operation Definition

An Operation is a pamola core processing unit that performs a specific analysis or transformation on data, producing metrics, visualizations, or transformed datasets.  The operation is located in the pamola core pamola core inside a specific module, which is also included in the specified package.

### 4.2 Operation Classes

Operations must inherit from one of the following base classes defined in pamola_core/utils/ops/op_base.py:

1. `BaseOperation`: Generic operation base class that provides:
   - Common logging functionality
   - Standard error handling
   - Progress tracking
   - Result processing
   - Directory preparation

2. `FieldOperation`: For operations that process specific fields, extending BaseOperation with:
   - Field name management
   - Field existence validation
   - Field-specific artifact naming

3. `DataFrameOperation`: For operations that process entire DataFrames, extending BaseOperation with:
   - Dataset validation
   - Cross-field analysis capabilities
   - DataFrame-level artifact management

When updating existing operations, maintain compatibility with these base classes rather than performing total rewrites. Focus on aligning interfaces and method signatures while preserving domain-specific logic.

### 4.3 Operation Properties

Each Operation must define the following properties:

| Property            | Description                                    | Required               | Default                |
| ------------------- | ---------------------------------------------- | ---------------------- | ---------------------- |
| `name`              | Name of the operation                          | Yes                    | -                      |
| `description`       | Description of what the operation does         | Yes                    | -                      |
| `field_name`        | Field being processed (for FieldOperation)     | Yes for FieldOperation | -                      |
| `parameters`        | Operation-specific parameters                  | No                     | {}                     |
| `status`            | Current status of the operation                | No                     | Not started            |
| `execution_time`    | Time taken for execution                       | No                     | None                   |
| `use_encryption`    | Whether to encrypt output                      | No                     | False                  |
| `use_vectorization` | Whether to use vectorized processing (dask)    | No                     | False                  |
| `scope`             | Fields to process (for multi-field operations) | No                     | All fields             |
| `input_datasets`    | Main and auxiliary datasets                    | No                     | {"main": main_dataset} |
| `output_format`     | Format for output files (csv, parquet, etc.)   | No                     | "csv"                  |
| `timestamp_format`  | Format for timestamps in filenames             | No                     | "%Y%m%d_%H%M%S"        |


### 4.4 Operation Methods

Each Operation must implement:

1. `execute(data_source, task_dir, reporter, progress_tracker=None, **kwargs)`:
    - Process data according to operation logic
    - Generate artifacts (JSON metrics, PNG visualizations, CSV data)
    - Return a standardized OperationResult object

Operations may also implement helper methods specific to their domain.

### 4.5 Operation Result


Each operation must return an `OperationResult` object containing:
1. Status (SUCCESS, WARNING, ERROR, SKIPPED)
2. List of artifacts produced (with paths, types, descriptions)
3. Metrics collected
4. Error message (if applicable)
5. Execution time

Logging for operations should be:
- Written to `{data_repository}/logs/{task_type}/{task_id}/{operation_id}.log`
- Include DEBUG, INFO, WARNING, and ERROR levels
- Log both start and end of operation execution
- Log parameters provided to the operation
- Log created artifacts
- Log any errors or warnings
- Use standardized logging through `pamola_core/utils/logging.py`

The operation should not manage its own log files directly but use the logging configuration provided by the task.

### 4.6 Operation Categories

Operations should be categorized by their domain and function:

1. **Profiling Operations**:
    
    - Completeness analysis
    - Uniqueness analysis
    - Categorical analysis
    - Numeric analysis
    - Text analysis
    - Multi-valued field analysis
    - Correlation analysis
    - Duplicate detection
2. **Transformation Operations**:
    
    - Data cleaning
    - Type conversion
    - Normalization
    - Encoding/Decoding
3. **Anonymization Operations**:
    
    - Suppression
    - Generalization
    - Masking
    - Perturbation
    - Synthetic data generation
4. **Attack Operations**:
    
    - Re-identification attacks
    - Linkage attacks
    - Inference attacks

### 4.7 Operation Registration

Operations must be registered in the operation registry to be discoverable and usable by tasks. Registration can be done:

1. Using the `@register` decorator
2. Explicitly calling `register_operation()`
3. Through the `discover_operations()` function that scans packages

## 5. Data Flow Requirements

### 5.1 Data Source Abstraction

Operations must use the `DataSource` class to access data, which provides:

1. Access to DataFrames by name
2. Access to file paths by name
3. Lazy loading of DataFrames from files
4. Static methods for creating from single DataFrames or files

### 5.2 Data Input/Output

All data I/O must use utilities from `pamola_core/utils/io.py`:

1. `read_full_csv()`: Read CSV files
2. `write_dataframe_to_csv()`: Write DataFrames to CSV
3. `read_json()` / `write_json()`: Read/write JSON files
4. Support for encrypted I/O when handling sensitive data

### 5.3 Data Formats

The system must support:

1. CSV files (primary format)
2. JSON files (for metrics and configurations)
3. Parquet files (optional, for large datasets)
4. PNG files (for visualizations)

### 5.4 Data Repository Structure

```
{data_repository}/
├── raw/                                  # Raw input data
├── processed/                            # Processed data
│   ├── {task_type_1}/                       # Grouped by task type i.e. profiling
│   │   ├── {task_id_1}/                        # Task-specific outputs
│   │   └── {task_id_2}/
│   └── {task_type_2}/
├── reports/                              # Execution reports
│   ├── {task_type_1}/
│   │   └── {task_id}.json
│   └── {task_type_2}/
└── logs/                                 # Execution logs
    ├── {task_type_1}/
    │   └── {task_id}/
    └── {task_type_2}/
```

## 6. Security Requirements

### 6.1 Data Encryption

For sensitive data:

1. Support AES-256 encryption for data at rest
2. Use a master key for the project
3. Encrypt individual task keys with the master key
4. Store encrypted keys in `{data_repository}/keys/`

### 6.2 Data Handling

1. Sanitize sensitive data from logs and reports
2. Provide secure deletion options for temporary files
3. Support for data masking in visualizations

## 7. Logging and Monitoring

### 7.1 Logging

1. Use standardized logging through `pamola_core/utils/logging.py`
2. Store logs in `{data_repository}/logs/{task_type}/{task_id}/`
3. Include timestamps, severity levels, and context information
4. Support for different log levels (DEBUG, INFO, WARNING, ERROR)

### 7.2 Progress Tracking

1. Use `ProgressTracker` from `pamola_core/utils/progress.py`
2. Report progress at operation level
3. Support for nested progress tracking

## 8. Configuration Requirements

### 8.1 Project Configuration

The project configuration file (`{project_root}/configs/prj_config.json`) must define:
1. Project metadata (name, description, version)
2. Data repository path
3. Default settings for tasks and operations
4. Pipeline definitions (sequences of tasks)
5. Security settings (encryption, access control)

### 8.2 Task Configuration

Task configuration includes:

1. Task metadata (id, type, description)
2. Input/output dataset specifications
3. Field selections for processing
4. Operation sequence and parameters
5. Reporting settings

### 8.3 Configuration Inheritance

Configuration should follow inheritance rules:

1. System defaults
2. Project configuration
3. Task type defaults
4. Task-specific configuration
5. Command-line overrides

## 9. Implementation Priorities (MVP)

### 9.1 Phase 1: Pamola Core Framework Refinement

1. Task Base Structure:
   - Create or refine `pamola_core/utils/tasks/base_task.py` to define the BaseTask class
   - Implement the task execution flow with operation chaining
   - Add configuration loading and validation
   - Create standardized task reporting

2. Operation Framework Updates:
   - Refine `pamola_core/utils/ops/op_base.py` to ensure consistent interfaces
   - Update `pamola_core/utils/ops/op_result.py` to include additional metrics
   - Enhance `pamola_core/utils/ops/op_data_source.py` for better file handling
   - Improve `pamola_core/utils/ops/op_registry.py` for operation discovery and creation

3. Utility Module Standardization:
   - Ensure `pamola_core/utils/io.py` handles all data I/O consistently
   - Update `pamola_core/utils/logging.py` for standardized logging
   - Enhance `pamola_core/utils/progress.py` for better progress tracking
   - Refine `pamola_core/utils/task_reporting.py` for comprehensive reports

### 9.2 Phase 2: Existing Operation Refinement

1. Profiling Operations:
   - Refactor existing profiling operations to use the updated base classes
   - Standardize artifact generation and storage
   - Improve error handling and reporting
   - Ensure consistent interface across all profiling operations

2. Fake Data Operations:
   - Update existing fake data generation operations
   - Standardize interfaces and parameter handling
   - Improve integration with the task framework

### 9.3 Future Work

1. Anonymization Operations
2. Attack Simulation
3. Pipeline Management
4. Web-based User Interface
5. Distributed Processing Support

## 10. Quality Assurance

### 10.1 Testing Requirements

1. Unit tests for individual operations
2. Integration tests for tasks
3. Test datasets and expected outputs
4. Performance benchmarks

### 10.2 Documentation Requirements

1. API documentation for all classes and methods
2. User guide for creating custom tasks and operations
3. Example configurations and tasks
4. System architecture documentation

## 11. Future Extensions

### 11.1 Planned Extensions

1. Web-based user interface for task configuration and monitoring
2. Support for distributed processing
3. Advanced privacy metrics and guarantees
4. Integration with external anonymization tools
5. Real-time monitoring and alerting

### 11.2 Extensibility Requirements

The system must be designed for extensibility:

1. Plugin architecture for custom operations
2. Well-defined interfaces for all components
3. Version compatibility mechanisms
4. Support for third-party libraries through adapters

## Appendix A: Operation Templates

### A.1 Generic Operation Template

```python
@register
class SampleOperation(BaseOperation):
    """
    Sample operation that demonstrates the standard structure.
    """
    
    def __init__(self, name="Sample Operation", description="A sample operation"):
        super().__init__(name, description)
    
    def execute(self, data_source, task_dir, reporter, progress_tracker=None, **kwargs):
        """Execute the operation."""
        # Create result object
        result = OperationResult()
        
        # Get main dataframe
        df = data_source.get_dataframe("main")
        if df is None:
            result.status = OperationStatus.ERROR
            result.error_message = "Main dataframe not found"
            return result
        
        # Process data
        # ...
        
        # Create artifacts
        metrics_path = task_dir / f"sample_metrics_{int(time.time())}.json"
        with open(metrics_path, 'w') as f:
            json.dump({"metric1": 100, "metric2": 200}, f)
        
        result.add_artifact("json", metrics_path, "Sample metrics")
        result.add_metric("rows_processed", len(df))
        
        return result
```

### A.2 Field Operation Template

```python
@register
class FieldAnalysisOperation(FieldOperation):
    """
    Analyzes a specific field in the dataset.
    """
    
    def __init__(self, field_name):
        super().__init__(field_name, f"Analysis of {field_name} field")
    
    def execute(self, data_source, task_dir, reporter, progress_tracker=None, **kwargs):
        """Execute the field analysis."""
        result = OperationResult()
        
        # Get main dataframe
        df = data_source.get_dataframe("main")
        if df is None:
            result.status = OperationStatus.ERROR
            result.error_message = "Main dataframe not found"
            return result
        
        # Check if field exists
        if self.field_name not in df.columns:
            result.status = OperationStatus.ERROR
            result.error_message = f"Field {self.field_name} not found in dataframe"
            return result
        
        # Analyze field
        field_data = df[self.field_name]
        # ...
        
        # Create artifacts
        # ...
        
        return result
```

## Appendix B: Task Template

```python
from pamola_core.utils.tasks.base_task import BaseTask
from pamola_core.utils.ops.op_data_source import DataSource
from pamola_core.utils.ops.op_registry import create_operation_instance

class SampleTask(BaseTask):
    """
    Sample task that demonstrates the standard structure.
    """
    
    def __init__(self):
        super().__init__(
            task_id="sample_task",
            task_type="profiling",
            description="A sample profiling task"
        )
    
    def execute(self):
        """Execute the task."""
        # Initialize data source
        data_source = DataSource.from_file_path(
            self.config.input_path,
            name="main"
        )
        
        # Create operations
        operations = [
            create_operation_instance("CompletenessOperation"),
            create_operation_instance("FieldAnalysisOperation", field_name="age"),
            create_operation_instance("FieldAnalysisOperation", field_name="name")
        ]
        
        # Execute operations
        for operation in operations:
            result = operation.run(
                data_source=data_source,
                task_dir=self.task_dir,
                reporter=self.reporter
            )
            
            if result.status == OperationStatus.ERROR:
                self.logger.error(f"Operation {operation.name} failed: {result.error_message}")
                return False
        
        # Generate report
        self.generate_report()
        
        return True

if __name__ == "__main__":
    task = SampleTask()
    success = task.run()
    exit(0 if success else 1)
```


## Appendix C. Implementation Checklist

### C.1 Code Style and Documentation
- [ ] Use English for all comments and documentation
- [ ] Include comprehensive docstrings for all classes and methods
- [ ] Follow PEP 8 style guidelines
- [ ] Include type hints for function parameters and return values
- [ ] Document class inheritance and relationships

### C.2 Utility Usage
- [ ] Use only `pamola_core/utils/io.py` for all file I/O operations
- [ ] Use only `pamola_core/utils/logging.py` for all logging
- [ ] Use only `pamola_core/utils/visualization.py` for creating visualizations
- [ ] Use only `pamola_core/utils/progress.py` for progress tracking
- [ ] Use standardized error handling and reporting

### C.3 Operation Implementation
- [ ] Inherit from appropriate base operation class
- [ ] Implement required methods (execute, etc.)
- [ ] Register operation with the registry
- [ ] Create comprehensive tests for each operation
- [ ] Include examples in documentation

### C.4 Task Implementation
- [ ] Inherit from BaseTask
- [ ] Define task metadata (id, type, description)
- [ ] Implement execute method
- [ ] Handle configuration loading
- [ ] Manage operation execution sequence
- [ ] Generate comprehensive reports

## Appendix D. Resources.
- **Base Architecture Files**:
    - `pamola_core/utils/ops/op_base.py` (as is)
    - `pamola_core/utils/ops/op_data_source.py` as is)
    - `pamola_core/utils/ops/op_registry.py` (as is)
    - `pamola_core/utils/ops/op_result.py` (as is)
    - `pamola_core/utils/tasks/base_task.py` (not realized yet)
- **Utility Modules**:
    - `pamola_core/utils/io.py` (for understanding file I/O patterns)
    - `pamola_core/utils/logging.py` (for logging standards)
    - `pamola_core/utils/progress.py` (to see how progress tracking works)
    - `pamola_core/utils/task_reporting.py` (for report generation)
- **Example Operation Implementations**:
    - At least one existing profiling operation (e.g., `pamola_core/profiling/analyzers/categorical.py`, `numeric.py`)
    - At least one existing field operation implementation
- **Example Task Implementations**:
    - An existing task script (e.g., `scripts/pipeline/profiling/profile_details.py`)
- **Configuration Examples**:
    - `configs/prj_config.json` (or whatever the current config file is)
    - Any task-specific configuration files
- **Project Structure**:
    - A complete directory listing to understand file organization

## Appendix E. Op* Refactoring

After reviewing the provided `op_*.py` modules, they already have a solid foundation but need some adjustments to fully align with the requirements in the SRS. Here are the specific changes needed for each file:

### 1. `pamola_core/utils/ops/op_base.py`

The base structure is strong, but needs these updates:

- **Add encryption and vectorization support**:
    - Add `use_encryption` and `use_vectorization` parameters to `__init__`
    - Add logic in `run()` to handle these parameters
- **Add scope management**:
    - Add support for processing multiple fields in `FieldOperation`
    - Add a `scope` parameter that can define which fields to process
- **Improve directory preparation**:
    - Add support for timestamp-based output directories
    - Add support for custom directory naming
- **Enhance error handling**:
    - Add more specific error types
    - Improve the context information in error messages

### 2. `pamola_core/utils/ops/op_data_source.py`

This module is mostly solid, but needs:

- **Enhance data handling capabilities**:
    - Add support for parquet files
    - Add methods for data sampling (for large datasets)
    - Add support for multi-file datasets
- **Improve caching**:
    - Add memory management for large datasets
    - Add option to control caching behavior
- **Add data validation**:
    - Add schema validation methods
    - Add data quality checking methods

### 3. `pamola_core/utils/ops/op_registry.py`

The registry system is comprehensive but could use:

- **Enhance category management**:
    - Add subcategories for operations
    - Add better classification for domain-specific operations
- **Improve versioning**:
    - Add version tracking for operations
    - Add compatibility checking between operation versions
- **Add dependency management**:
    - Add support for operation dependencies
    - Add validation of dependency chains

### 4. `pamola_core/utils/ops/op_result.py`

The result structure is good but needs:

- **Add more status types**:
    - Add `PARTIAL_SUCCESS` status for partial completions
    - Add `PENDING` status for asynchronous operations
- **Enhance metrics**:
    - Add standard metric categories
    - Add support for nested metrics
- **Improve artifact management**:
    - Add grouping for related artifacts
    - Add more metadata for artifacts (size, creation time, etc.)
    - Add validation for artifact existence