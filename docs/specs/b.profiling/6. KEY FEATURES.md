# 6. Распределение функциональности по модулям профилирования PAMOLA.CORE

## 6.1. Введение

Данный документ описывает распределение функциональности по модулям в рамках рефакторинга пакета `pamola_core.profiling`. Целью является структурирование существующей функциональности и определение недостающих компонентов для сохранения и расширения возможностей анализа данных.

Проект PAMOLA.CORE (Privacy-Preserving AI Data Processors) направлен на анонимизацию резюме. Профилирование данных — важный этап этого процесса, позволяющий понять структуру, качество и взаимосвязи в данных перед их анонимизацией.

## 6.2. Структура пакета после рефакторинга

После рефакторинга пакет `pamola_core.profiling` будет иметь следующую структуру:

```
pamola_core/profiling/
├── commons/
│   ├── base.py           # Базовые классы и интерфейсы
│   ├── data_types.py     # Определения типов данных
│   └── helpers.py        # Вспомогательные функции
├── analyzers/
│   ├── categorical.py    # Анализ категориальных полей
│   ├── numeric.py        # Анализ числовых полей
│   ├── text.py           # Анализ текстовых полей
│   ├── longtext.py       # Расширенный анализ длинных текстов
│   ├── date.py           # Анализ дат
│   ├── email.py          # Анализ email-адресов
│   ├── phone.py          # Анализ телефонных номеров
│   ├── mvf.py            # Анализ многозначных полей
│   ├── correlation.py    # Анализ корреляций
│   └── group.py          # Анализ групп записей
└── reporters.py          # Формирование отчетов
```

## 6.3. Функциональность модулей

### 6.3.1. Commons

#### 6.3.1.1. base.py

**Назначение**: Определение базовых классов и интерфейсов для анализаторов и операций.

**Основные классы**:

- `BaseAnalyzer`: Абстрактный класс для всех анализаторов
- `BaseOperation`: Абстрактный класс для операций профилирования
- `AnalysisResult`: Контейнер для результатов анализа

**Ключевые методы**:

- `BaseAnalyzer.analyze(df, field_name, **kwargs)`: Выполнение анализа и возврат результатов
- `BaseOperation.execute(df, reporter, profile_type, **kwargs)`: Выполнение операции с отчетностью
- `AnalysisResult.add_artifact(artifact_type, artifact_path, description)`: Добавление артефакта

#### 6.3.1.2. data_types.py

**Назначение**: Определение типов данных, перечислений и констант.

**Основные компоненты**:

- `DataType`: Перечисление типов данных (NUMERIC, CATEGORICAL, TEXT, EMAIL и т.д.)
- `PrivacyLevel`: Перечисление уровней приватности полей
- `ProfilerConfig`: Константы и настройки профилирования

**Константы**:

- `DEFAULT_TOP_N`: Число значений для вывода в распределениях
- `DEFAULT_MIN_FREQUENCY`: Минимальная частота для включения в словарь
- `DATE_FORMATS`: Стандартные форматы дат

#### 6.3.1.3. helpers.py

**Назначение**: Вспомогательные функции для обработки и анализа данных.

**Основные функции**:

- `infer_data_type(series)`: Определение типа данных
- `convert_numpy_types(obj)`: Конвертация типов для JSON
- `prepare_field_for_analysis(df, field_name)`: Подготовка поля к анализу

**Примечание**: Функции `is_valid_email(value)` и `extract_email_domain(email)` перенесены в модуль email.py для более четкого разделения ответственности. Аналогичные функции для телефонов перенесены в phone.py.

### 6.3.2. Analyzers

#### 6.3.2.1. categorical.py

**Назначение**: Анализ категориальных полей.

**Основные классы**:

- `CategoricalAnalyzer`: Анализатор для категориальных полей
- `CategoricalFieldOperation`: Операция для анализа категориальных полей

**Операции**:

- Анализ распределения значений
- Создание частотного словаря
- Выявление топ-значений
- Расчет показателей разнообразия (энтропия)

**Обрабатываемые поля**:

- education_level, relocation, road_time_type, business_trip_readiness
- first_name, last_name, middle_name
- gender, area_name, metro_station_name, has_vehicle

**Артефакты**:

- `{field_name}_stats.json`: Статистика и распределения
- `dictionaries/{field_name}_dictionary.csv`: Словарь значений
- `{field_name}_distribution.png`: Визуализация распределения

#### 6.3.2.2. numeric.py

**Назначение**: Анализ числовых полей.

**Основные классы**:

- `NumericAnalyzer`: Анализатор для числовых полей
- `NumericFieldOperation`: Операция для анализа числовых полей

**Операции**:

- Расчет базовых статистик (min, max, mean, median)
- Определение выбросов методом IQR
- Распределение по диапазонам значений
- Нормализация и тестирование нормальности

**Обрабатываемые поля**:

- salary: зарплата
- Количественные метрики в других полях

**Артефакты**:

- `{field_name}_stats.json`: Статистические показатели
- `{field_name}_distribution.png`: Гистограмма распределения
- `{field_name}_boxplot.png`: Диаграмма размаха с выбросами

#### 6.3.2.3. text.py

**Назначение**: Анализ текстовых полей средней длины.

**Основные классы**:

- `TextAnalyzer`: Анализатор для текстовых полей
- `TextFieldOperation`: Операция для анализа текстовых полей

**Операции**:

- Анализ длины текста
- Извлечение и анализ ключевых слов
- Частотный анализ слов
- Базовый лингвистический анализ

**Обрабатываемые поля**:

- post: должность
- Другие текстовые поля средней длины

**Артефакты**:

- `{field_name}_stats.json`: Статистика и анализ текста
- `{field_name}_keywords_dictionary.csv`: Словарь ключевых слов
- `{field_name}_wordcloud.png`: Облако слов
- `{field_name}_length_distribution.png`: Распределение длин

#### 6.3.2.4. longtext.py

**Назначение**: Расширенный анализ длинных текстов.

**Основные классы**:

- `LongTextAnalyzer`: Анализатор для длинных текстов
- `LongTextFieldOperation`: Операция для анализа длинных текстовых полей

**Операции**:

- Тематическое моделирование
- Извлечение именованных сущностей (NER)
- Анализ тональности
- Классификация текста по категориям

**Обрабатываемые поля**:

- Описания, длинные тексты

**Артефакты**:

- `{field_name}_stats.json`: Результаты глубокого анализа
- `dictionaries/{field_name}_entities.csv`: Извлеченные сущности
- `{field_name}_topics_distribution.png`: Распределение тем

#### 6.3.2.5. date.py

**Назначение**: Анализ полей с датами.

**Основные классы**:

- `DateAnalyzer`: Анализатор для дат
- `DateFieldOperation`: Операция для анализа полей с датами

**Операции**:

- Парсинг и валидация дат
- Распределение по периодам (годам, месяцам)
- Выявление аномальных дат
- Расчет возраста для дат рождения

**Обрабатываемые поля**:

- birth_day: дата рождения
- Другие поля с датами

**Артефакты**:

- `{field_name}_stats.json`: Статистика и анализ дат
- `{field_name}_anomalies.csv`: Список аномальных дат
- `{field_name}_year_distribution.png`: Распределение по годам
- `age_distribution.png`: Распределение возрастов (для дат рождения)

#### 6.3.2.6. email.py

**Назначение**: Специализированный анализ email-адресов.

**Основные классы**:

- `EmailAnalyzer`: Анализатор для email-полей
- `EmailOperation`: Операция для анализа email-полей

**Операции**:

- Валидация корректности email
- Извлечение и анализ доменов
- Выявление паттернов (name.surname и т.п.)
- Анализ рисков деанонимизации

**Основные функции**:

- `is_valid_email(value)`: Проверка корректности email
- `extract_email_domain(email)`: Извлечение домена из email

**Обрабатываемые поля**:

- email: поле с email-адресами

**Артефакты**:

- `email_stats.json`: Статистика и анализ email
- `dictionaries/email_domains_dictionary.csv`: Словарь доменов
- `email_domains_distribution.png`: Распределение доменов
- `email_patterns_distribution.png`: Распределение паттернов

#### 6.3.2.7. phone.py

**Назначение**: Специализированный анализ телефонных номеров.

**Основные классы**:

- `PhoneAnalyzer`: Анализатор для телефонных номеров
- `PhoneOperation`: Операция для анализа телефонных номеров

**Операции**:

- Парсинг номеров на компоненты
- Определение страны и оператора
- Анализ комментариев (мессенджеры)
- Валидация и стандартизация номеров

**Основные функции**:

- `is_valid_phone(value)`: Проверка корректности телефона
- `extract_phone_components(phone)`: Выделение компонентов номера
- `identify_country_code(phone)`: Определение кода страны
- `identify_operator_code(phone)`: Определение кода оператора

**Обрабатываемые поля**:

- home_phone: домашний телефон
- work_phone: рабочий телефон
- cell_phone: мобильный телефон

**Артефакты**:

- `{phone_field}_stats.json`: Статистика и анализ
- `dictionaries/{phone_field}_country_codes_dictionary.csv`: Коды стран
- `dictionaries/{phone_field}_operator_codes_dictionary.csv`: Коды операторов
- `{phone_field}_country_codes_distribution.png`: Распределение кодов стран
- `{phone_field}_messenger_mentions.png`: Упоминания мессенджеров

#### 6.3.2.8. mvf.py

**Назначение**: Анализ многозначных полей.

**Основные классы**:

- `MVFAnalyzer`: Анализатор для многозначных полей
- `MVFOperation`: Операция для анализа многозначных полей
- `MVFConverter`: Стандартизированный преобразователь MVF-полей

**Операции**:

- Парсинг многозначных полей
- Анализ отдельных значений и их распределения
- Анализ комбинаций значений
- Анализ количества значений на запись

**Основные функции**:

- `parse_mvf(value)`: Парсинг многозначного поля
- `standardize_mvf_format(value)`: Приведение к стандартному формату
- `handle_invalid_mvf(value)`: Обработка некорректных форматов

**Обрабатываемые поля**:

- work_schedules: графики работы
- employments: типы занятости
- driver_license_types: типы водительских прав

**Артефакты**:

- `{mvf_field}_stats.json`: Статистика и анализ
- `dictionaries/{mvf_field}_values_dictionary.csv`: Словарь значений
- `dictionaries/{mvf_field}_combinations_dictionary.csv`: Словарь комбинаций
- `{mvf_field}_values_distribution.png`: Распределение значений
- `{mvf_field}_combinations_distribution.png`: Распределение комбинаций
- `{mvf_field}_value_counts_distribution.png`: Распределение количества значений

#### 6.3.2.9. correlation.py

**Назначение**: Анализ взаимосвязей между полями.

**Основные классы**:

- `CorrelationAnalyzer`: Анализатор корреляций
- `CorrelationOperation`: Операция для анализа корреляций

**Операции**:

- Расчет корреляции Пирсона для числовых полей
- Расчет V Крамера для категориальных полей
- Расчет точечно-бисериальной корреляции
- Создание корреляционной матрицы

**Обрабатываемые поля**:

- Пары полей для анализа (например, salary и education_level)
- Группы полей для корреляционной матрицы

**Артефакты**:

- `{field1}_{field2}_correlation.json`: Анализ корреляции пары полей
- `correlation_matrix.json`: Корреляционная матрица
- `correlation_matrix_heatmap.png`: Тепловая карта корреляций
- `{field1}_{field2}_correlation_scatter.png`: Диаграмма рассеяния

#### 6.3.2.10. group.py

**Назначение**: Анализ групп записей с одинаковым идентификатором.

**Основные классы**:

- `GroupAnalyzer`: Анализатор для групп записей
- `GroupOperation`: Операция для анализа групп

**Операции**:

- Расчет вариабельности внутри групп
- Взвешенная вариабельность по нескольким полям
- Анализ размеров групп
- Выявление аномальных групп

**Улучшения**:

- Улучшенная адресация на уровне подтаблиц
- Добавление метаданных о происхождении группы
- Включение контекста использования группы
- Анализ частоты изменений полей внутри групп с одинаковым resume_id
- Поддержка "схлопывания" структуры и последующего восстановления

**Обрабатываемые поля**:

- resume_id: идентификатор резюме (группирующее поле)
- Наборы полей для анализа вариабельности

**Артефакты**:

- `group_variation.json` или `group_variation_set{N}.json`: Анализ вариабельности
- `dictionaries/group_variation_details.csv`: Детальная информация
- `group_variation_distribution.png`: Распределение вариабельности
- `group_size_distribution.png`: Распределение размеров групп

### 6.3.3. Reporters

#### 6.3.3.1. reporters.py

**Назначение**: Организация и структурирование отчетов о результатах профилирования.

**Основные классы**:

- `ProfileReporter`: Класс для формирования отчетов профилирования

**Методы**:

- `report_field_analysis(field_name, result, artifacts)`: Отчет об анализе поля
- `collect_report(results)`: Сбор результатов в структурированный отчет
- `format_summary_report()`: Формирование сводного отчета
- `save_report(path)`: Сохранение отчета

**Расширения**:

- Включение идентификатора задачи (task_id)
- Добавление описания задачи (task_description)
- Фиксация даты и времени запуска
- Отслеживание метрик производительности
- Мониторинг статуса выполнения каждой операции
- Подробный список всех артефактов с их описанием
- Описание каждой операции на уровне библиотеки
- Дифференциация между словарями с частотностью и простыми списками значений

**Артефакты**:

- `{task_id}_report.json`: JSON-отчет о профилировании
- `html/{task_id}_report.html`: HTML-отчет с визуализациями (опционально)

## 6.4. Обработка полей по типам

### 6.4.1. Идентификационные данные

| Поле        | Тип            | Модуль анализа | Операции                                  |
| ----------- | -------------- | -------------- | ----------------------------------------- |
| resume_id   | ID             | group.py       | Группировка, анализ вариабельности        |
| first_name  | Категориальное | categorical.py | Распределение, словарь, частоты           |
| last_name   | Категориальное | categorical.py | Распределение, словарь, частоты           |
| middle_name | Категориальное | categorical.py | Распределение, словарь, частоты           |
| birth_day   | Дата           | date.py        | Распределение по годам, аномалии, возраст |
| gender      | Категориальное | categorical.py | Распределение, частоты                    |
| UID         | ID             | -              | Консистентность, дубликаты                |
| file_as     | Текст          | text.py        | Анализ формата, соответствие именам       |

### 6.4.2. Контактные данные

|Поле|Тип|Модуль анализа|Операции|
|---|---|---|---|
|email|Email|email.py|Валидация, домены, паттерны|
|home_phone|Телефон|phone.py|Коды стран, операторы, мессенджеры|
|work_phone|Телефон|phone.py|Коды стран, операторы, мессенджеры|
|cell_phone|Телефон|phone.py|Коды стран, операторы, мессенджеры|

### 6.4.3. Профессиональные детали

|Поле|Тип|Модуль анализа|Операции|
|---|---|---|---|
|post|Текст|text.py|Длина, ключевые слова, облако слов|
|education_level|Категориальное|categorical.py|Распределение, словарь|
|salary|Числовое|numeric.py|Статистика, выбросы, диапазоны|
|salary_currency|Категориальное|categorical.py|Распределение, связь с salary|
|area_name|Категориальное|categorical.py|Распределение, топ-значения|
|relocation|Категориальное|categorical.py|Распределение|
|metro_station_name|Категориальное|categorical.py|Распределение по городам|
|road_time_type|Категориальное|categorical.py|Распределение|
|business_trip_readiness|Категориальное|categorical.py|Распределение|
|work_schedules|MVF|mvf.py|Значения, комбинации, количество|
|employments|MVF|mvf.py|Значения, комбинации, количество|
|driver_license_types|MVF|mvf.py|Значения, комбинации, количество|
|has_vehicle|Категориальное|categorical.py|Распределение, связь с правами|

### 6.4.4. Корреляции между полями

|Пара полей|Модуль анализа|Операции|
|---|---|---|
|salary × education_level|correlation.py|Корреляция, диаграмма|
|salary × area_name|correlation.py|Корреляция, диаграмма|
|relocation × area_name|correlation.py|Корреляция|
|business_trip_readiness × relocation|correlation.py|Корреляция|
|work_schedules × employments|correlation.py|Корреляция|
|driver_license_types × has_vehicle|correlation.py|Корреляция|
|email × cell_phone|correlation.py|Корреляция|
|home_phone × work_phone|correlation.py|Корреляция|
|cell_phone × work_phone|correlation.py|Корреляция|

## 6.5. Соответствие между старыми и новыми модулями

|Старый модуль/функция|Новый модуль|Комментарий|
|---|---|---|
|basic.py:analyze_completeness|commons/helpers.py|Базовый анализ полноты|
|basic.py:analyze_uniqueness|commons/helpers.py|Базовый анализ уникальности|
|basic.py:analyze_frequency|categorical.py|Частотный анализ для категориальных|
|basic.py:analyze_numeric_stats|numeric.py|Анализ числовых полей|
|categorical.py:analyze_categorical_field|categorical.py|Анализ категориальных полей|
|categorical.py:save_categorical_dictionary|categorical.py|Сохранение словаря категорий|
|contact_analysis.py:analyze_email_field|email.py|Анализ email|
|contact_analysis.py:analyze_phone_field|phone.py|Анализ телефонов|
|date_analysis.py:analyze_birth_dates|date.py|Анализ дат рождения|
|group_variation.py:analyze_resume_group_variation|group.py|Анализ вариабельности групп|
|mvf_analysis.py:analyze_mvf_field|mvf.py|Анализ многозначных полей|
|text_analysis.py:analyze_text_field|text.py|Анализ текстовых полей|
|correlation_analysis.py:analyze_field_correlations|correlation.py|Анализ корреляций|
|visualization.py:plot_*|utils/visualization.py|Вынесено из пакета профилирования|

## 6.6. Функциональность по скриптам задач

### 6.6.1. profile_ident.py

**Назначение**: Профилирование таблицы IDENTIFICATION (идентификационные данные).

**Обрабатываемые поля**:

- resume_id, first_name, last_name, middle_name, birth_day, gender, UID, file_as

**Основные операции**:

1. Анализ полноты и уникальности данных (включая пропуски)
2. Анализ имен и фамилий (распределения, словари)
3. Анализ полов (распределение, статистика)
4. Анализ дат рождения (распределение по годам, возрасты, аномалии, пропуски)
5. Анализ количества резюме на человека
6. Анализ UID и его консистентности
7. Анализ поля file_as
8. Анализ дубликатов
9. Анализ вариабельности групп по resume_id

**Используемые модули после рефакторинга**:

- commons/helpers.py: Базовые метрики (полнота, уникальность)
- categorical.py: Анализ имен, фамилий, полов
- date.py: Анализ дат рождения
- group.py: Анализ групп резюме
- text.py: Анализ поля file_as
- reporters.py: Формирование отчета

**Параметры задачи**:

- Включение/отключение меток времени в именах файлов
- Настройка уровня логирования
- Опция очистки директории с артефактами перед выполнением
- Интеграция с cli.py для управления параметрами

**Генерируемые артефакты**:

|Артефакт|Тип|Описание|Анализ пропусков|
|---|---|---|---|
|completeness.json|JSON|Процент заполненности для каждого поля|Основной фокус анализа|
|completeness_{timestamp}.png|PNG|Визуализация полноты данных по всем полям|Показывает % пропусков|
|uniqueness.json|JSON|Процент уникальных значений для каждого поля|Не учитывает пропуски|
|name_fields_analysis.json|JSON|Подробная статистика по именам, фамилиям, отчествам|Содержит count_null|
|dictionaries/first_name_dictionary.csv|CSV|Частотный словарь имен|Не включает пропуски|
|dictionaries/last_name_dictionary.csv|CSV|Частотный словарь фамилий|Не включает пропуски|
|dictionaries/middle_name_dictionary.csv|CSV|Частотный словарь отчеств|Не включает пропуски|
|first_name_distribution_{timestamp}.png|PNG|Распределение топ-20 имен|Не отображает пропуски|
|last_name_distribution_{timestamp}.png|PNG|Распределение топ-20 фамилий|Не отображает пропуски|
|middle_name_distribution_{timestamp}.png|PNG|Распределение топ-20 отчеств|Не отображает пропуски|
|gender_distribution.json|JSON|Распределение значений пола|Включает подсчет null|
|gender_distribution_{timestamp}.png|PNG|Визуализация распределения полов|Включает null если есть|
|birth_date_stats.json|JSON|Статистика по датам рождения|Включает null_dates, их количество и %|
|birth_year_distribution_{timestamp}.png|PNG|Распределение по годам рождения|Не отображает пропуски|
|birth_date_anomalies_{timestamp}.csv|CSV|Список аномальных дат с причинами|Включает пустые даты как аномалии|
|resume_counts.json|JSON|Статистика по количеству резюме на человека|Учитывает связанные записи|
|resume_count_distribution_{timestamp}.png|PNG|Визуализация распределения резюме по людям|Не отображает пропуски|
|name_based_resume_counts.json|JSON|Альтернативный анализ по именам/фамилиям|Исключает записи с пропусками|
|name_based_resume_distribution_{timestamp}.png|PNG|Визуализация альтернативного анализа|Не отображает пропуски|
|uid_consistency.json|JSON|Анализ консистентности UID|Учитывает записи с пропусками|
|uid_generation_check.json|JSON|Проверка алгоритма генерации UID|Исключает записи с пропусками|
|file_as_analysis.json|JSON|Анализ поля file_as|Включает подсчет null и пустых строк|
|person_duplicates.json|JSON|Анализ дубликатов по персональным данным|Исключает записи с критическими пропусками|
|uid_duplicates.json|JSON|Анализ дубликатов по UID|Исключает записи с пропусками UID|
|resume_id_duplicates.json|JSON|Анализ дубликатов по resume_id|Исключает записи с пропусками ID|
|resume_group_variation_{timestamp}.json|JSON|Анализ вариабельности групп резюме|Учитывает пропуски как значения|
|dictionaries/resume_group_variation_details.csv|CSV|Детальная информация о вариабельности по группам|Содержит детали о пропусках|
|resume_group_variation_distribution.png|PNG|Визуализация распределения вариабельности|Не отображает детализацию пропусков|

### 6.6.2. profile_details.py

**Назначение**: Профилирование таблицы RESUME_DETAILS (профессиональные данные).

**Обрабатываемые поля**:

- post, education_level, salary, salary_currency, area_name, metro_station_name, relocation, road_time_type, business_trip_readiness, work_schedules, employments, driver_license_types, has_vehicle

**Основные операции**:

1. Анализ полноты и уникальности данных (включая пропуски)
2. Анализ поля post (должность, включая пропуски)
3. Анализ уровня образования (распределение, пропуски)
4. Анализ зарплаты и валюты (включая нулевые значения и пропуски) 
5. Анализ географических данных (город, станция метро, пропуски) 
6. Анализ категориальных полей (relocation, road_time_type, business_trip_readiness) 
7.  Анализ многозначных полей (work_schedules, employments, driver_license_types)
8. Анализ поля has_vehicle (включая пропуски) 
9. Анализ вариабельности внутри групп резюме 
10. Корреляционный анализ между полями 
11. Анализ дубликатов

**Используемые модули после рефакторинга**:

- commons/helpers.py: Базовые метрики
- categorical.py: Категориальные поля
- numeric.py: Анализ зарплаты
- text.py: Анализ должности
- mvf.py: Анализ многозначных полей
- correlation.py: Анализ взаимосвязей
- group.py: Анализ вариабельности
- reporters.py: Формирование отчета

**Параметры задачи**:

- Включение/отключение меток времени в именах файлов
- Настройка уровня логирования
- Опция очистки директории с артефактами перед выполнением
- Интеграция с cli.py для управления параметрами

**Генерируемые артефакты**:

|Артефакт|Тип|Описание|Анализ пропусков|
|---|---|---|---|
|completeness.json|JSON|Процент заполненности каждого поля|Основной фокус анализа|
|completeness_{timestamp}.png|PNG|Визуализация полноты данных по полям|Явно показывает % пропусков|
|uniqueness.json|JSON|Процент уникальных значений для полей|Не учитывает пропуски|
|post_stats.json|JSON|Анализ поля должности|Включает count_null, % пропусков|
|dictionaries/post_dictionary.csv|CSV|Частотный словарь должностей|Не включает пропуски|
|post_distribution_{timestamp}.png|PNG|Распределение топ должностей|Не отображает пропуски|
|post_wordcloud_{timestamp}.png|PNG|Облако слов из должностей|Создается только для заполненных значений|
|post_keywords_{timestamp}.png|PNG|Распределение ключевых слов|Не отображает пропуски|
|post_length_distribution_{timestamp}.png|PNG|Распределение длин текстов|Исключает пропуски|
|education_level_stats.json|JSON|Статистика по уровням образования|Включает count_null и % пропусков|
|dictionaries/education_level_dictionary.csv|CSV|Словарь уровней образования|Не включает пропуски|
|education_level_distribution_{timestamp}.png|PNG|Визуализация уровней образования|Может включать категорию "не указано"|
|salary_stats.json|JSON|Статистика по зарплате|Отдельно анализирует null_count и zero_count|
|salary_groups_distribution_{timestamp}.png|PNG|Распределение по группам зарплат|Включает категорию "0"|
|salary_log_distribution_{timestamp}.png|PNG|Логарифмическое распределение|Исключает нули и пропуски|
|salary_currency_stats.json|JSON|Анализ валюты зарплаты|Включает null_count и анализ соответствия зарплате|
|dictionaries/salary_currency_dictionary.csv|CSV|Словарь валют|Не включает пропуски|
|salary_currency_distribution_{timestamp}.png|PNG|Распределение валют|Может включать категорию "не указано"|
|area_name_stats.json|JSON|Анализ регионов/городов|Включает count_null и % пропусков|
|dictionaries/area_name_dictionary.csv|CSV|Словарь городов|Не включает пропуски|
|area_name_distribution_{timestamp}.png|PNG|Распределение топ-10 городов|Не отображает пропуски|
|metro_station_stats.json|JSON|Анализ станций метро|Включает детальный анализ пропусков по городам|
|dictionaries/metro_stations_dictionary.csv|CSV|Словарь станций метро|Не включает пропуски|
|metro_stations_{city}_{timestamp}.png|PNG|Распределение станций в городе|Не отображает пропуски|
|relocation_stats.json|JSON|Анализ готовности к переезду|Включает count_null и % пропусков|
|dictionaries/relocation_dictionary.csv|CSV|Словарь значений|Не включает пропуски|
|relocation_distribution_{timestamp}.png|PNG|Визуализация распределения|Может включать категорию "не указано"|
|road_time_type_stats.json|JSON|Анализ времени в пути|Включает count_null и % пропусков|
|dictionaries/road_time_type_dictionary.csv|CSV|Словарь значений|Не включает пропуски|
|road_time_type_distribution_{timestamp}.png|PNG|Визуализация распределения|Может включать "не указано"|
|business_trip_readiness_stats.json|JSON|Анализ готовности к командировкам|Включает count_null и % пропусков|
|dictionaries/business_trip_readiness_dictionary.csv|CSV|Словарь значений|Не включает пропуски|
|business_trip_readiness_distribution_{timestamp}.png|PNG|Визуализация распределения|Может включать "не указано"|
|work_schedules_stats.json|JSON|Анализ графиков работы (MVF)|Включает count_null, empty_arrays|
|dictionaries/work_schedules_values_dictionary.csv|CSV|Словарь отдельных значений|Не включает пропуски|
|dictionaries/work_schedules_combinations_dictionary.csv|CSV|Словарь комбинаций|Не включает пропуски|
|work_schedules_values_distribution_{timestamp}.png|PNG|Распределение значений|Не отображает пропуски|
|work_schedules_combinations_distribution_{timestamp}.png|PNG|Распределение комбинаций|Не отображает пропуски|
|work_schedules_value_counts_distribution_{timestamp}.png|PNG|Распределение количества значений|Включает категорию "0" для пустых|
|employments_stats.json|JSON|Анализ типов занятости (MVF)|Включает count_null, empty_arrays|
|dictionaries/employments_values_dictionary.csv|CSV|Словарь отдельных значений|Не включает пропуски|
|dictionaries/employments_combinations_dictionary.csv|CSV|Словарь комбинаций|Не включает пропуски|
|employments_values_distribution_{timestamp}.png|PNG|Распределение значений|Не отображает пропуски|
|employments_combinations_distribution_{timestamp}.png|PNG|Распределение комбинаций|Не отображает пропуски|
|employments_value_counts_distribution_{timestamp}.png|PNG|Распределение количества значений|Включает категорию "0" для пустых|
|driver_license_types_stats.json|JSON|Анализ прав (MVF)|Включает count_null, empty_arrays|
|dictionaries/driver_license_types_values_dictionary.csv|CSV|Словарь отдельных значений|Не включает пропуски|
|dictionaries/driver_license_types_combinations_dictionary.csv|CSV|Словарь комбинаций|Не включает пропуски|
|driver_license_types_values_distribution_{timestamp}.png|PNG|Распределение значений|Не отображает пропуски|
|driver_license_types_combinations_distribution_{timestamp}.png|PNG|Распределение комбинаций|Не отображает пропуски|
|driver_license_types_value_counts_distribution_{timestamp}.png|PNG|Распределение количества значений|Включает категорию "0" для пустых|
|has_vehicle_stats.json|JSON|Анализ наличия транспорта|Включает null_count и % пропусков|
|has_vehicle_distribution_{timestamp}.png|PNG|Визуализация распределения|Включает категорию "не указано"|
|group_variation_set1.json|JSON|Анализ вариабельности (набор 1)|Учитывает пропуски как отдельные значения|
|dictionaries/group_variation_details_set1.csv|CSV|Детальная информация о вариабельности|Содержит детали о пропусках|
|group_variation_distribution_set1.png|PNG|Визуализация распределения|Не отображает детализацию пропусков|
|group_variation_set2.json|JSON|Анализ вариабельности (набор 2)|Учитывает пропуски как отдельные значения|
|dictionaries/group_variation_details_set2.csv|CSV|Детальная информация о вариабельности|Содержит детали о пропусках|
|group_variation_distribution_set2.png|PNG|Визуализация распределения|Не отображает детализацию пропусков|
|{field1}_{field2}_correlation.json|JSON|Анализ корреляции пары полей|Исключает записи с пропусками в обоих полях|
|correlation_matrix.json|JSON|Корреляционная матрица|Исключает записи с пропусками в анализируемых полях|
|correlation_matrix_heatmap_{timestamp}.png|PNG|Тепловая карта корреляций|Не отображает влияние пропусков|
|duplicates.json|JSON|Анализ дубликатов|Может учитывать NULL как значение в поиске дубликатов|
|resume_id_groups.json|JSON|Статистика по группам resume_id|Включает анализ пропусков в группах|
|resume_id_group_size_distribution_{timestamp}.png|PNG|Распределение размеров групп|Не отображает детали пропусков|

### 6.6.3. profile_contacts.py

**Назначение**: Профилирование таблицы CONTACTS (контактные данные).

**Обрабатываемые поля**:

- email, home_phone, work_phone, cell_phone

**Основные операции**:

1. Анализ полноты и уникальности данных (включая пропуски)
2. Анализ email (домены, валидность, пропуски)
3. Анализ телефонных номеров (коды стран, операторы, мессенджеры, пропуски)
4. Анализ доступности контактов (комбинации наличия разных контактов)
5. Анализ вариабельности групп (изменение контактов в пределах одного resume_id)
6. Анализ общих контактов между разными резюме
7. Анализ рисков деанонимизации (включая уникальность контактов)
8. Корреляционный анализ между контактными данными

**Используемые модули после рефакторинга**:

- commons/helpers.py: Базовые метрики
- email.py: Анализ email
- phone.py: Анализ телефонных номеров
- group.py: Анализ вариабельности групп
- correlation.py: Корреляционный анализ
- reporters.py: Формирование отчета

**Параметры задачи**:

- Включение/отключение меток времени в именах файлов
- Настройка уровня логирования
- Опция очистки директории с артефактами перед выполнением
- Интеграция с cli.py для управления параметрами

**Генерируемые артефакты**:

|Артефакт|Тип|Описание|Анализ пропусков|
|---|---|---|---|
|completeness.json|JSON|Процент заполненности каждого поля|Основной фокус анализа|
|completeness_{timestamp}.png|PNG|Визуализация полноты данных по полям|Явно показывает % пропусков|
|uniqueness.json|JSON|Процент уникальных значений для полей|Не учитывает пропуски|
|email_stats.json|JSON|Анализ email|Включает null_count, формат: valid, invalid, null|
|dictionaries/email_domains_dictionary.csv|CSV|Частотный словарь доменов|Не включает пропуски|
|email_domains_distribution_{timestamp}.png|PNG|Распределение топ-15 доменов|Не отображает пропуски|
|home_phone_stats.json|JSON|Анализ домашнего телефона|Включает null_count, формат: valid, invalid, null|
|dictionaries/home_phone_country_codes_dictionary.csv|CSV|Словарь кодов стран|Не включает пропуски|
|home_phone_country_codes_distribution_{timestamp}.png|PNG|Распределение кодов стран|Не отображает пропуски|
|dictionaries/home_phone_operator_codes_dictionary.csv|CSV|Словарь кодов операторов|Не включает пропуски|
|home_phone_operator_codes_distribution_{timestamp}.png|PNG|Распределение кодов операторов|Не отображает пропуски|
|dictionaries/home_phone_messenger_references_dictionary.csv|CSV|Словарь упоминаний мессенджеров|Не включает пропуски|
|home_phone_messenger_mentions_{timestamp}.png|PNG|Распределение мессенджеров|Не отображает пропуски|
|work_phone_stats.json|JSON|Анализ рабочего телефона|Включает null_count, формат: valid, invalid, null|
|dictionaries/work_phone_country_codes_dictionary.csv|CSV|Словарь кодов стран|Не включает пропуски|
|work_phone_country_codes_distribution_{timestamp}.png|PNG|Распределение кодов стран|Не отображает пропуски|
|dictionaries/work_phone_operator_codes_dictionary.csv|CSV|Словарь кодов операторов|Не включает пропуски|
|work_phone_operator_codes_distribution_{timestamp}.png|PNG|Распределение кодов операторов|Не отображает пропуски|
|dictionaries/work_phone_messenger_references_dictionary.csv|CSV|Словарь упоминаний мессенджеров|Не включает пропуски|
|work_phone_messenger_mentions_{timestamp}.png|PNG|Распределение мессенджеров|Не отображает пропуски|
|cell_phone_stats.json|JSON|Анализ мобильного телефона|Включает null_count, формат: valid, invalid, null|
|dictionaries/cell_phone_country_codes_dictionary.csv|CSV|Словарь кодов стран|Не включает пропуски|
|cell_phone_country_codes_distribution_{timestamp}.png|PNG|Распределение кодов стран|Не отображает пропуски|
|dictionaries/cell_phone_operator_codes_dictionary.csv|CSV|Словарь кодов операторов|Не включает пропуски|
|cell_phone_operator_codes_distribution_{timestamp}.png|PNG|Распределение кодов операторов|Не отображает пропуски|
|dictionaries/cell_phone_messenger_references_dictionary.csv|CSV|Словарь упоминаний мессенджеров|Не включает пропуски|
|cell_phone_messenger_mentions_{timestamp}.png|PNG|Распределение мессенджеров|Не отображает пропуски|
|contact_availability.json|JSON|Анализ доступности контактов|Фокус на комбинациях наличия/отсутствия контактов|
|contact_availability_distribution_{timestamp}.png|PNG|Визуализация категорий доступности|Включает анализ пропусков|
|group_variation.json|JSON|Анализ вариабельности контактов|Учитывает пропуски как отдельные значения|
|dictionaries/group_variation_details.csv|CSV|Детальная информация о вариабельности|Содержит детали о пропусках|
|group_variation_distribution.png|PNG|Визуализация распределения|Не отображает детализацию пропусков|
|shared_contacts.json|JSON|Анализ общих контактов|Исключает контакты с пропусками|
|dictionaries/shared_emails_dictionary.csv|CSV|Словарь общих email|Только для заполненных email|
|dictionaries/shared_phones_dictionary.csv|CSV|Словарь общих телефонов|Только для заполненных телефонов|
|anonymization_risk.json|JSON|Анализ рисков деанонимизации|Анализирует отдельно пропуски и заполненные значения|
|email_anonymization_risk_{timestamp}.png|PNG|Визуализация рисков для email|Не отображает пропуски|
|email_cell_phone_correlation.json|JSON|Корреляция email и мобильного|Исключает записи с пропусками в обоих полях|
|home_phone_work_phone_correlation.json|JSON|Корреляция домашнего и рабочего|Исключает записи с пропусками в обоих полях|
|cell_phone_work_phone_correlation.json|JSON|Корреляция мобильного и рабочего|Исключает записи с пропусками в обоих полях|

## 6.7. Потенциальные улучшения и дополнительная функциональность

### 6.7.1. Создание сэмплов данных

**Функциональность**:

- Генерация репрезентативных выборок из больших наборов данных
- Создание аннотированных примеров для анализа аномалий
- Экспорт интересных случаев для последующего изучения

**Реализация**:

- Добавить в commons/helpers.py функции:
    - `extract_samples(df, field_name, conditions, limit=10)`
    - `extract_anomalies(df, field_name, anomaly_detector, limit=10)`

### 6.7.2. Расширенная обработка текстовых полей

**Функциональность**:

- Классификация текстов по категориям
- Анализ настроения (sentiment analysis)
- Извлечение конкретных типов информации (должности, компании, технологии)

**Реализация**:

- Расширить text.py и longtext.py:
    - `detect_technologies(text)`: Выявление технологий в описаниях
    - `classify_text(text, categories)`: Классификация текста

### 6.7.3. Профилирование приватности

**Функциональность**:

- Расчет метрик k-анонимности
- Оценка рисков деанонимизации
- Выявление идентифицирующих комбинаций полей

**Реализация**:

- Добавить новый модуль privacy.py:
    - `calculate_k_anonymity(df, quasi_identifiers)`
    - `assess_identification_risk(df, field_combinations)`

### 6.7.4. Интеграция машинного обучения

**Функциональность**:

- Кластеризация резюме для выявления паттернов
- Предиктивный анализ для выявления зависимостей
- Автоматическое определение типов полей

**Реализация**:

- Добавить новый модуль ml.py:
    - `cluster_data(df, features, n_clusters)`
    - `predict_relationships(df, target, features)`

### 6.7.5. Анализ геоданных

**Функциональность**:

- Географическая визуализация распределения
- Анализ расстояний и транспортной доступности
- Картирование данных по регионам

**Реализация**:

- Добавить новый модуль geo.py:
    - `visualize_geographic_distribution(df, location_field)`
    - `analyze_distances(df, from_field, to_field)`

## 6.8. Рекомендации по реализации недостающих модулей

### 6.8.1. Приоритизация разработки

**Высокий приоритет**:

- mvf.py: Необходим для анализа многозначных полей, встречающихся во всех скриптах
- group.py: Важен для анализа вариабельности групп резюме
- correlation.py: Обеспечивает понимание взаимосвязей между полями

**Средний приоритет**:

- text.py: Необходим для анализа текстовых полей вроде post
- privacy.py (новый): Для оценки рисков деанонимизации

**Низкий приоритет**:

- longtext.py: Для расширенного анализа длинных текстов
- ml.py (новый): Для интеграции методов машинного обучения

### 6.8.2. Интеграция с существующими скриптами задач

Для обеспечения плавного перехода со старой архитектуры на новую рекомендуется:

1. **Создать адаптеры для совместимости**:
    
    - Реализовать промежуточный слой, который транслирует вызовы старых функций в вызовы новых
    - Обеспечить одинаковый формат возвращаемых данных для сохранения совместимости
2. **Поэтапная миграция скриптов**:
    
    - Начать с переписывания profile_ident.py как наиболее простого скрипта
    - Затем переработать profile_contacts.py, использующий специализированные анализаторы
    - В последнюю очередь переписать profile_details.py, который использует наибольшее число модулей
3. **Параллельное тестирование**:
    
    - Запускать старую и новую версии скриптов на одних и тех же данных
    - Сравнивать результаты для гарантии сохранения функциональности

### 6.8.3. Стандартизация интерфейсов

Для обеспечения единообразия и упрощения поддерживаемости рекомендуется:

1. **Унифицировать интерфейсы всех анализаторов**:
    
    - Один базовый метод analyze() с общими параметрами
    - Одинаковая структура возвращаемых результатов
    - Обязательная обработка пропусков во всех анализаторах
2. **Стандартизировать структуру артефактов**:
    
    - Одинаковый формат JSON для результатов анализа:
        
        ```json
        {  "field_name": "field_name",  "data_type": "data_type",  "total_records": 100000,  "null_count": 1234,  "null_percentage": 1.234,  "stats": {    // Специфические статистики для конкретного типа  },  "artifacts": [    {"type": "png", "path": "path/to/artifact.png", "description": "..."}  ]}
        ```
        
    - Общая структура для CSV-словарей:
        - Обязательные колонки: value, count, percentage
        - Опциональные колонки зависят от типа поля
    - Согласованный стиль визуализаций с унифицированными цветами и шрифтами
3. **Централизовать конфигурацию**:
    
    - Хранить все параметры анализа в централизованной конфигурации
    - Обеспечить возможность переопределения параметров на уровне задачи
    - Создать стандартные профили настроек для разных типов анализа
4. **Стандартизировать обработку пропусков**:
    
    - Каждый анализатор должен учитывать и анализировать пропуски
    - Единый способ представления пропусков в отчетах и визуализациях
    - Сохранение метаданных о пропущенных значениях во всех артефактах

### 6.8.4. Реализация недостающих модулей

#### 6.8.4.1. mvf.py

**Ключевые компоненты для реализации**:

- Функция `parse_mvf()` должна учитывать различные форматы представления MVF:
    - Строковые представления массивов: `"['Value1', 'Value2']"`
    - JSON-строки: `"[\"Value1\", \"Value2\"]"`
    - Пустые массивы: `"[]"` или `null`
    - Некорректные форматы с обработкой ошибок
- Методы для анализа уникальных значений и популярных комбинаций
- Функциональность для сравнения распределений в разных MVF-полях
- Обработка пустых значений и некорректных форматов
- Анализ количества значений на запись (0, 1, 2, 3+)
- Расчет частоты совместной встречаемости значений

**Структура класса**:

```python
class MVFAnalyzer(BaseAnalyzer):
    """Analyzer for multi-valued fields."""
    
    def analyze(self, df, field_name, **kwargs):
        """Analyze a multi-valued field."""
        # Обработка пропусков
        null_count = df[field_name].isna().sum()
        
        # Парсинг многозначных полей
        parsed_values = df[field_name].apply(parse_mvf)
        
        # Анализ пустых массивов отдельно от NULL
        empty_arrays_count = (parsed_values.apply(len) == 0).sum() - null_count
        
        # Анализ отдельных значений
        values_analysis = self._analyze_values(parsed_values)
        
        # Анализ комбинаций
        combinations_analysis = self._analyze_combinations(parsed_values)
        
        # Анализ количества значений
        value_counts_distribution = self._analyze_value_counts(parsed_values)
        
        # Формирование результатов
        return AnalysisResult(
            stats={
                'total_records': len(df),
                'null_count': int(null_count),
                'null_percentage': round((null_count / len(df)) * 100, 2),
                'empty_arrays_count': int(empty_arrays_count),
                'empty_arrays_percentage': round((empty_arrays_count / len(df)) * 100, 2),
                'values_analysis': values_analysis,
                'combinations_analysis': combinations_analysis,
                'value_counts_distribution': value_counts_distribution
            },
            field_name=field_name,
            data_type=DataType.MVF.value
        )
```

#### 6.8.4.2. group.py

**Ключевые компоненты для реализации**:

- Функционал для взвешенного анализа вариабельности по разным полям:
    - Обработка числовых, категориальных, текстовых полей
    - Специальная обработка MVF полей
    - Особая обработка пропусков (как значения или исключение)
- Методы выявления аномальных групп с высокой вариабельностью
- Анализ изменений значений полей внутри групп во времени
- Поддержка выявления паттернов в изменениях
- Детальное логирование изменений внутри групп
- Улучшенная адресация на уровне подтаблиц
- Метаданные о происхождении и контексте использования групп
- Поддержка "схлопывания" структуры и последующего восстановления



**Структура класса** :

```python
class GroupAnalyzer(BaseAnalyzer):
    """Analyzer for groups of records with the same identifier."""
    
    def analyze(self, df, group_field, fields_weights, **kwargs):
        """
        Analyze variation within groups identified by group_field.
        
        Parameters:
        -----------
        df : pd.DataFrame
            DataFrame to analyze
        group_field : str
            Field to group by (e.g., 'resume_id')
        fields_weights : Dict[str, float]
            Dictionary of fields and their weights for variation calculation
        """
        # Проверка наличия полей
        for field in list(fields_weights.keys()) + [group_field]:
            if field not in df.columns:
                return AnalysisResult(
                    stats={'error': f"Field {field} not found in DataFrame"},
                    field_name=group_field,
                    data_type='group_variation'
                )
        
        # Группировка данных
        groups = df.groupby(group_field)
        
        # Статистика по группам
        group_stats = {
            'total_groups': len(groups),
            'min_group_size': groups.size().min(),
            'max_group_size': groups.size().max(),
            'mean_group_size': groups.size().mean(),
            'analyzed_groups': 0,
            'fields_analyzed': list(fields_weights.keys()),
            'group_metadata': kwargs.get('group_metadata', {}),
            'group_context': kwargs.get('group_context', ''),
            'subtable_name': kwargs.get('subtable_name', '')
        }
        
        # Анализ вариабельности
        variation_results = []
        
        for group_id, group_df in groups:
            if len(group_df) > 1:  # Анализируем только группы размером > 1
                variation = self._calculate_weighted_variation(
                    group_df, 
                    fields_weights,
                    handle_nulls=kwargs.get('handle_nulls', 'as_value')
                )
                
                variation_results.append({
                    'group_id': group_id,
                    'size': len(group_df),
                    'variation': variation,
                    'field_variations': {
                        field: self._calculate_field_variation(
                            group_df, 
                            field,
                            handle_nulls=kwargs.get('handle_nulls', 'as_value')
                        )
                        for field in fields_weights
                    },
                    'change_frequency': self._calculate_change_frequency(group_df, fields_weights),
                    'metadata': self._extract_group_metadata(group_df, kwargs.get('metadata_fields', []))
                })
        
        # Обновление статистики
        group_stats['analyzed_groups'] = len(variation_results)
        
        # Статистика по вариабельности
        if variation_results:
            variations = [r['variation'] for r in variation_results]
            group_stats['overall_stats'] = {
                'min_variation': min(variations),
                'max_variation': max(variations),
                'mean_variation': sum(variations) / len(variations),
                'median_variation': sorted(variations)[len(variations) // 2]
            }
            
            # Распределение вариабельности
            group_stats['variation_distribution'] = self._calculate_variation_distribution(variations)
            
            # Группировка по паттернам изменений
            group_stats['change_patterns'] = self._identify_change_patterns(variation_results)
            
            # Анализ "схлопываемости" групп
            if kwargs.get('analyze_collapsibility', False):
                group_stats['collapsibility_analysis'] = self._analyze_collapsibility(
                    variation_results,
                    threshold=kwargs.get('collapsibility_threshold', 0.2)
                )
        
        return (
            AnalysisResult(
                stats=group_stats,
                field_name=group_field,
                data_type='group_variation'
            ),
            pd.DataFrame(variation_results)
        )
```

#### 6.8.4.3. correlation.py

**Ключевые компоненты для реализации**:

- Поддержка различных типов корреляций в зависимости от типов полей:
    - Корреляция Пирсона для числовых полей
    - V Крамера для категориальных полей
    - Точечно-бисериальная корреляция для категориальных и числовых полей
    - Корреляционное отношение для категорий и чисел
- Текстовая интерпретация полученных значений корреляции
- Функции построения корреляционной матрицы и ее визуализации
- Выявление наиболее значимых корреляций
- Специальная обработка пропусков (попарное исключение, заполнение)

**Структура класса**:

```python
class CorrelationAnalyzer(BaseAnalyzer):
    """Analyzer for correlations between fields."""
    
    def analyze(self, df, field1, field2, **kwargs):
        """
        Analyze correlation between two fields.
        
        Parameters:
        -----------
        df : pd.DataFrame
            DataFrame to analyze
        field1 : str
            First field name
        field2 : str
            Second field name
        """
        from scipy import stats
        
        # Проверка наличия полей
        for field in [field1, field2]:
            if field not in df.columns:
                return AnalysisResult(
                    stats={'error': f"Field {field} not found in DataFrame"},
                    field_name=f"{field1}_{field2}",
                    data_type='correlation'
                )
        
        # Определение типов полей
        is_numeric1 = pd.api.types.is_numeric_dtype(df[field1])
        is_numeric2 = pd.api.types.is_numeric_dtype(df[field2])
        is_categorical1 = not is_numeric1
        is_categorical2 = not is_numeric2
        
        # Предобработка пропусков в зависимости от типов полей
        null_handling = kwargs.get('null_handling', 'drop')
        working_df = self._preprocess_nulls(df, field1, field2, null_handling)
        
        # Расчет корреляции в зависимости от типов полей
        if is_numeric1 and is_numeric2:
            # Корреляция Пирсона для двух числовых полей
            method = 'pearson'
            correlation, p_value = stats.pearsonr(working_df[field1], working_df[field2])
        elif is_categorical1 and is_categorical2:
            # V Крамера для категориальных полей
            method = 'cramers_v'
            correlation = self._calculate_cramers_v(working_df, field1, field2)
            p_value = None  # Для V Крамера отдельно вычисляем хи-квадрат и p-value
        elif is_numeric1 and is_categorical2:
            # Корреляционное отношение
            method = 'correlation_ratio'
            correlation = self._calculate_correlation_ratio(working_df, field1, field2)
            p_value = None
        elif is_categorical1 and is_numeric2:
            # Корреляционное отношение
            method = 'correlation_ratio'
            correlation = self._calculate_correlation_ratio(working_df, field2, field1)
            p_value = None
        
        # Интерпретация корреляции
        interpretation = self._interpret_correlation(correlation, method)
        
        # Формирование результатов
        result_stats = {
            'field1': field1,
            'field2': field2,
            'method': method,
            'correlation_coefficient': correlation,
            'p_value': p_value,
            'interpretation': interpretation,
            'sample_size': len(working_df),
            'null_handling': null_handling,
            'records_with_nulls': len(df) - len(working_df)
        }
        
        return AnalysisResult(
            stats=result_stats,
            field_name=f"{field1}_{field2}",
            data_type='correlation'
        )
    
    def create_correlation_matrix(self, df, fields, **kwargs):
        """Create a correlation matrix for multiple fields."""
        import numpy as np
        import pandas as pd
        
        # Матрица для хранения результатов
        matrix = pd.DataFrame(index=fields, columns=fields)
        methods = pd.DataFrame(index=fields, columns=fields)
        
        # Заполняем матрицу корреляций
        for i, field1 in enumerate(fields):
            for j, field2 in enumerate(fields):
                if i == j:
                    # Диагональ матрицы - корреляция поля с самим собой
                    matrix.loc[field1, field2] = 1.0
                    methods.loc[field1, field2] = 'self'
                elif i < j:
                    # Вычисляем корреляцию и заполняем верхний треугольник
                    result = self.analyze(df, field1, field2, **kwargs)
                    
                    if 'error' in result.stats:
                        matrix.loc[field1, field2] = np.nan
                        methods.loc[field1, field2] = 'error'
                    else:
                        matrix.loc[field1, field2] = result.stats['correlation_coefficient']
                        methods.loc[field1, field2] = result.stats['method']
                        
                        # Для симметричных методов заполняем и нижний треугольник
                        if result.stats['method'] in ['pearson', 'cramers_v']:
                            matrix.loc[field2, field1] = result.stats['correlation_coefficient']
                            methods.loc[field2, field1] = result.stats['method']
                elif methods.loc[field2, field1] not in ['correlation_ratio', 'error', None]:
                    # Копируем значения из верхнего треугольника для симметричных методов
                    matrix.loc[field1, field2] = matrix.loc[field2, field1]
                    methods.loc[field1, field2] = methods.loc[field2, field1]
                else:
                    # Для несимметричных методов вычисляем отдельно
                    result = self.analyze(df, field1, field2, **kwargs)
                    
                    if 'error' in result.stats:
                        matrix.loc[field1, field2] = np.nan
                        methods.loc[field1, field2] = 'error'
                    else:
                        matrix.loc[field1, field2] = result.stats['correlation_coefficient']
                        methods.loc[field1, field2] = result.stats['method']
        
        # Выявление значимых корреляций
        significant_correlations = self._identify_significant_correlations(matrix, methods, threshold=kwargs.get('significance_threshold', 0.3))
        
        return matrix, methods, significant_correlations
```

#### 6.8.4.4. text.py

**Ключевые компоненты для реализации**:

- Анализ длины текста на уровне символов, слов, предложений:
    - Учет пустых строк и пропусков
    - Разбивка по диапазонам длин
    - Выявление аномально длинных/коротких текстов
- Извлечение ключевых слов с учетом стоп-слов и частотности:
    - Поддержка разных языков
    - Возможность указания пользовательских стоп-слов
    - Поддержка N-грамм
- Языконезависимые методы анализа для поддержки многоязычных резюме
- Генерация облаков слов и других визуализаций
- Базовый лингвистический анализ (части речи, энтропия текста)

**Структура класса**:

```python
class TextAnalyzer(BaseAnalyzer):
    """Analyzer for text fields of moderate length."""
    
    def analyze(self, df, field_name, **kwargs):
        """
        Analyze a text field.
        
        Parameters:
        -----------
        df : pd.DataFrame
            DataFrame to analyze
        field_name : str
            Field name to analyze
        **kwargs:
            min_word_length : int
                Minimum word length to include (default: 3)
            top_n : int
                Number of top keywords to include (default: 50)
            min_frequency : int
                Minimum frequency for keywords (default: 2)
            stopwords : Set[str]
                Custom stopwords (optional)
            language : str
                Language for stopwords (default: 'russian')
        """
        import re
        from collections import Counter
        
        # Проверка наличия поля
        if field_name not in df.columns:
            return AnalysisResult(
                stats={'error': f"Field {field_name} not found in DataFrame"},
                field_name=field_name,
                data_type=DataType.TEXT.value
            )
        
        # Параметры анализа
        min_word_length = kwargs.get('min_word_length', 3)
        top_n = kwargs.get('top_n', 50)
        min_frequency = kwargs.get('min_frequency', 2)
        language = kwargs.get('language', 'russian')
        custom_stopwords = kwargs.get('stopwords', set())
        
        # Базовая статистика
        total_records = len(df)
        null_count = df[field_name].isna().sum()
        empty_strings = (df[field_name] == '').sum()
        non_empty_count = total_records - null_count - empty_strings
        
        # Анализ длины текста
        length_stats = self._analyze_text_length(df, field_name)
        
        # Извлечение и анализ слов
        keywords = self._extract_keywords(
            df, 
            field_name, 
            min_word_length, 
            min_frequency,
            language,
            custom_stopwords
        )
        
        # Выбор топ-N ключевых слов
        top_keywords = {
            k: v for k, v in sorted(
                keywords.items(), 
                key=lambda item: item[1], 
                reverse=True
            )[:top_n]
        }
        
        # Лингвистический анализ
        linguistic_analysis = {}
        if kwargs.get('perform_linguistic_analysis', False):
            linguistic_analysis = self._perform_linguistic_analysis(
                df, 
                field_name,
                language=language
            )
        
        # Формирование результатов
        return AnalysisResult(
            stats={
                'total_records': total_records,
                'null_count': int(null_count),
                'null_percentage': round((null_count / total_records) * 100, 2),
                'empty_strings': int(empty_strings),
                'empty_percentage': round((empty_strings / total_records) * 100, 2),
                'non_empty_count': int(non_empty_count),
                'non_empty_percentage': round((non_empty_count / total_records) * 100, 2),
                'length_analysis': length_stats,
                'top_keywords': top_keywords,
                'language': language,
                'linguistic_analysis': linguistic_analysis
            },
            field_name=field_name,
            data_type=DataType.TEXT.value
        )
```

## 6.9. Общие паттерны обработки пропусков

### 6.9.1. Стандартный подход к анализу пропусков

Во всех анализаторах должен быть реализован единый подход к анализу пропусков:

```python
# Базовая статистика пропусков
total_records = len(df)
null_count = df[field_name].isna().sum()
null_percentage = round((null_count / total_records) * 100, 2)

# Добавление в результаты
stats = {
    'total_records': total_records,
    'null_count': int(null_count),
    'null_percentage': null_percentage,
    # ... другие статистики
}
```

### 6.9.2. Специфические паттерны для разных типов полей

**Категориальные поля**:

- Учет пустых строк отдельно от NULL
- Возможность включения NULL как категории в распределение

**Числовые поля**:

- Отдельный учет нулевых значений и NULL
- Отдельная статистика с исключением NULL и с их учетом

**Многозначные поля (MVF)**:

- Различение между NULL и пустыми массивами (`[]`)
- Учет количества записей с пустыми массивами

**Текстовые поля**:

- Различение между NULL, пустыми строками и пробельными строками
- Отдельная статистика длины для непустых значений

### 6.9.3. Визуализация с учетом пропусков

Рекомендуемые подходы к визуализации с учетом пропусков:

- Для диаграмм распределения: добавление отдельной категории "Не указано" с другим цветом
- Для числовых распределений: указание % пропусков на диаграмме без включения в основное распределение
- Для корреляционных матриц: цветовое кодирование отсутствия данных для корреляций

## 6.10. Заключение

Предложенная структура модулей профилирования обеспечивает более четкое разделение ответственности, уменьшает дублирование кода и упрощает поддерживаемость системы. Каждый модуль отвечает за конкретный тип полей и реализует специализированную логику анализа, соответствующую особенностям этих полей.

При реализации недостающих модулей важно сохранить существующую функциональность, обеспечив совместимость с текущими скриптами задач. В то же время, новая архитектура открывает возможности для реализации дополнительных методов анализа и расширения функциональности системы.

Особое внимание следует уделить обработке пропусков, которая должна быть стандартизирована во всех модулях и анализаторах. Это обеспечит полноту анализа и улучшит понимание качества данных, что критически важно для последующих этапов анонимизации.

Поэтапный подход к миграции и тщательное тестирование на каждом этапе помогут обеспечить плавный переход со старой архитектуры на новую без потери функциональности и качества анализа данных.